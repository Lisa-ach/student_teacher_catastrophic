{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "infrared-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-washer",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "minimal-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(32),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "fashion_mnist_train = torchvision.datasets.FashionMNIST(root=\"fmnist/\", download=False, transform=all_transforms)\n",
    "fashion_mnist_test = torchvision.datasets.FashionMNIST(root=\"fmnist/\", download=False, transform=all_transforms, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-cartoon",
   "metadata": {},
   "source": [
    "## Construct Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opened-defensive",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianlee/envs/cata/lib/python3.8/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist_train_splits = [torch.utils.data.Subset(fashion_mnist_train, torch.where(fashion_mnist_train.train_labels == i)[0]) for i in range(10)]\n",
    "fashion_mnist_test_splits = [torch.utils.data.Subset(fashion_mnist_test, torch.where(fashion_mnist_test.train_labels == i)[0]) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "super-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fashion_mnist_train_split_dataloaders = [torch.utils.data.DataLoader(fashion_mnist_split, batch_size=1, shuffle=True) for fashion_mnist_split in fashion_mnist_train_splits]\n",
    "# fashion_mnist_test_split_dataloaders = [torch.utils.data.DataLoader(fashion_mnist_split, batch_size=1, shuffle=True) for fashion_mnist_split in fashion_mnist_test_splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-december",
   "metadata": {},
   "source": [
    "## Dataset Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "insured-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dataset_mix(first_binary_classification_dataset, second_binary_classification_dataset, alpha: float):\n",
    "    \"\"\"OLD: Mixing on probability of sampling\"\"\"\n",
    "    random_indices_subset_1 = torch.bernoulli(alpha * torch.ones(len(first_binary_classification_dataset)))\n",
    "    random_indices_subset_2 = torch.bernoulli((1 - alpha) * torch.ones(len(second_binary_classification_dataset)))\n",
    "    subset_1 = torch.utils.data.Subset(first_binary_classification_dataset, torch.where(random_indices_subset_1)[0])\n",
    "    subset_2 = torch.utils.data.Subset(second_binary_classification_dataset, torch.where(random_indices_subset_2)[0])\n",
    "    concatenated_dataset = torch.utils.data.ConcatDataset([subset_1, subset_2])\n",
    "    return concatenated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "unavailable-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_01 = torch.utils.data.ConcatDataset([fashion_mnist_train_splits[0], fashion_mnist_train_splits[1]])\n",
    "dataset_78 = torch.utils.data.ConcatDataset([fashion_mnist_train_splits[7], fashion_mnist_train_splits[8]])\n",
    "\n",
    "dataset_01_test = torch.utils.data.ConcatDataset([fashion_mnist_test_splits[0], fashion_mnist_test_splits[1]])\n",
    "dataset_78_test = torch.utils.data.ConcatDataset([fashion_mnist_test_splits[7], fashion_mnist_test_splits[8]])\n",
    "\n",
    "dataloader_01 = torch.utils.data.DataLoader(dataset_01, batch_size=1, shuffle=True)\n",
    "dataloader_01_test = torch.utils.data.DataLoader(dataset_01_test, batch_size=1, shuffle=True)\n",
    "\n",
    "dataloader_78 = torch.utils.data.DataLoader(dataset_78, batch_size=1, shuffle=True)\n",
    "dataloader_78_test = torch.utils.data.DataLoader(dataset_78_test, batch_size=1, shuffle=True)\n",
    "\n",
    "target_mapping = {0: torch.Tensor([-1.]), 1: torch.Tensor([1.]), 7: torch.Tensor([-1.]), 8: torch.Tensor([1.])}\n",
    "label_mapping = {0: -1, 1: 1, 7: -1, 8: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-marine",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "activated-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 32 * 32\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self._layer1 = nn.Linear(input_dim, 8)\n",
    "        \n",
    "        self._layer2a = nn.Linear(8, 1)\n",
    "        self._layer2b = nn.Linear(8, 1)\n",
    "        \n",
    "        self._task = 0\n",
    "        \n",
    "    def switch(self, task_index):\n",
    "        self._task = task_index\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self._layer1(x))\n",
    "        \n",
    "        if self._task == 0:\n",
    "            y = self._layer2a(x)\n",
    "        elif self._task == 1:\n",
    "            y = self._layer2b(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-weekly",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "received-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader_1, dataloader_2, alpha, network, epochs, lr, target_mapping):\n",
    "    \n",
    "    loss_function = nn.MSELoss()\n",
    "    optimiser = torch.optim.SGD(params=network.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        epoch_loss = []\n",
    "        \n",
    "        for (x_1, y_1), (x_2, y_2) in zip(iter(dataloader_1), iter(dataloader_2)):\n",
    "            \n",
    "            mixed_input = alpha * x_1 + (1 - alpha) * x_2\n",
    "            \n",
    "            mapped_y_1 = target_mapping[y_1.item()]\n",
    "            mapped_y_2 = target_mapping[y_2.item()]\n",
    "            \n",
    "            mixed_label = torch.Tensor([np.sign(alpha * mapped_y_1 + (1 - alpha) * mapped_y_2)])\n",
    "            \n",
    "            prediction = network(mixed_input)\n",
    "            loss = loss_function(prediction.flatten(), mixed_label)\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "                    \n",
    "            epoch_loss.append(loss.item())\n",
    "        \n",
    "        print(f\"Epoch {e + 1}/{epochs} loss: {np.mean(epoch_loss)}\")\n",
    "        losses.append(np.mean(epoch_loss))\n",
    "            \n",
    "    return losses, network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-darkness",
   "metadata": {},
   "source": [
    "## Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "upset-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader_1, dataloader_2, alpha, network, target_mapping):\n",
    "    \n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    test_loss = []\n",
    "    correct_instances = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x_1, y_1), (x_2, y_2) in zip(iter(dataloader_1), iter(dataloader_2)):\n",
    "\n",
    "            mixed_input = alpha * x_1 + (1 - alpha) * x_2\n",
    "\n",
    "            mapped_y_1 = target_mapping[y_1.item()]\n",
    "            mapped_y_2 = target_mapping[y_2.item()]\n",
    "\n",
    "            mixed_label = torch.Tensor([np.sign(alpha * mapped_y_1 + (1 - alpha) * mapped_y_2)])\n",
    "\n",
    "            prediction = network(mixed_input)\n",
    "            loss = loss_function(prediction.flatten(), mixed_label)\n",
    "            correct = (torch.sign(prediction) == mixed_label).item()\n",
    "\n",
    "            test_loss.append(loss.item())\n",
    "            correct_instances.append(correct)\n",
    "\n",
    "    return np.mean(test_loss), sum(correct_instances) / len(correct_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-distinction",
   "metadata": {},
   "source": [
    "## Task 1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "described-gambling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 loss: 0.1588658816785105\n",
      "Epoch 2/100 loss: 0.09111098771674078\n",
      "Epoch 3/100 loss: 0.08195455870012515\n",
      "Epoch 4/100 loss: 0.07787328904034473\n",
      "Epoch 5/100 loss: 0.07425716059198796\n",
      "Epoch 6/100 loss: 0.06767421221543218\n",
      "Epoch 7/100 loss: 0.06338211366825493\n",
      "Epoch 8/100 loss: 0.06273099924613831\n",
      "Epoch 9/100 loss: 0.06017847271035386\n",
      "Epoch 10/100 loss: 0.05795161803787949\n",
      "Epoch 11/100 loss: 0.05687095136886982\n",
      "Epoch 12/100 loss: 0.05293232476017251\n",
      "Epoch 13/100 loss: 0.05290230780007759\n",
      "Epoch 14/100 loss: 0.05279714095429693\n",
      "Epoch 15/100 loss: 0.052533395233592975\n",
      "Epoch 16/100 loss: 0.05150259036078027\n",
      "Epoch 17/100 loss: 0.04952641708518081\n",
      "Epoch 18/100 loss: 0.048490900612972475\n",
      "Epoch 19/100 loss: 0.0474836511129615\n",
      "Epoch 20/100 loss: 0.0478142855072321\n",
      "Epoch 21/100 loss: 0.04585369263976691\n",
      "Epoch 22/100 loss: 0.04605596386574837\n",
      "Epoch 23/100 loss: 0.04372176106968829\n",
      "Epoch 24/100 loss: 0.04403360571295555\n",
      "Epoch 25/100 loss: 0.04247090278413621\n",
      "Epoch 26/100 loss: 0.043401514857254936\n",
      "Epoch 27/100 loss: 0.04237214965636491\n",
      "Epoch 28/100 loss: 0.04328730728754567\n",
      "Epoch 29/100 loss: 0.04198443373092409\n",
      "Epoch 30/100 loss: 0.04155275275552438\n",
      "Epoch 31/100 loss: 0.04125935129589425\n",
      "Epoch 32/100 loss: 0.04041955726953306\n",
      "Epoch 33/100 loss: 0.040344844517970074\n",
      "Epoch 34/100 loss: 0.03925280589729385\n",
      "Epoch 35/100 loss: 0.040417498843847446\n",
      "Epoch 36/100 loss: 0.038956435142223765\n",
      "Epoch 37/100 loss: 0.040012612716314\n",
      "Epoch 38/100 loss: 0.039882899191301904\n",
      "Epoch 39/100 loss: 0.03910132381812979\n",
      "Epoch 40/100 loss: 0.03880450749142061\n",
      "Epoch 41/100 loss: 0.03890594118941121\n",
      "Epoch 42/100 loss: 0.03811012622972386\n",
      "Epoch 43/100 loss: 0.038228057694640875\n",
      "Epoch 44/100 loss: 0.03846080867676962\n",
      "Epoch 45/100 loss: 0.036990679219168036\n",
      "Epoch 46/100 loss: 0.037291256354106725\n",
      "Epoch 47/100 loss: 0.037132814200053085\n",
      "Epoch 48/100 loss: 0.03674583374505504\n",
      "Epoch 49/100 loss: 0.035905908769464145\n",
      "Epoch 50/100 loss: 0.03778598268611581\n",
      "Epoch 51/100 loss: 0.037087473826699295\n",
      "Epoch 52/100 loss: 0.03649100980031374\n",
      "Epoch 53/100 loss: 0.03559031395949471\n",
      "Epoch 54/100 loss: 0.03509936410950665\n",
      "Epoch 55/100 loss: 0.03579543152401924\n",
      "Epoch 56/100 loss: 0.035682644622085584\n",
      "Epoch 57/100 loss: 0.035150942384633066\n",
      "Epoch 58/100 loss: 0.03604974670547595\n",
      "Epoch 59/100 loss: 0.03497066613687698\n",
      "Epoch 60/100 loss: 0.034912338751364055\n",
      "Epoch 61/100 loss: 0.035087921888766534\n",
      "Epoch 62/100 loss: 0.034502579527003745\n",
      "Epoch 63/100 loss: 0.035771146026147574\n",
      "Epoch 64/100 loss: 0.034403704379891306\n",
      "Epoch 65/100 loss: 0.03469324871529567\n",
      "Epoch 66/100 loss: 0.034425846891988854\n",
      "Epoch 67/100 loss: 0.0349210218523239\n",
      "Epoch 68/100 loss: 0.034848740255641844\n",
      "Epoch 69/100 loss: 0.03366153625234851\n",
      "Epoch 70/100 loss: 0.03307717227720496\n",
      "Epoch 71/100 loss: 0.03331700122167156\n",
      "Epoch 72/100 loss: 0.03350198767634546\n",
      "Epoch 73/100 loss: 0.033256895429183496\n",
      "Epoch 74/100 loss: 0.03291902960157536\n",
      "Epoch 75/100 loss: 0.03274977172196965\n",
      "Epoch 76/100 loss: 0.03266008085289314\n",
      "Epoch 77/100 loss: 0.03348410636911887\n",
      "Epoch 78/100 loss: 0.03203019418943597\n",
      "Epoch 79/100 loss: 0.032491027603391405\n",
      "Epoch 80/100 loss: 0.031777206095367966\n",
      "Epoch 81/100 loss: 0.032851278620730624\n",
      "Epoch 82/100 loss: 0.03362088966214163\n",
      "Epoch 83/100 loss: 0.03284091415591524\n",
      "Epoch 84/100 loss: 0.032621918704530704\n",
      "Epoch 85/100 loss: 0.032284342022816506\n",
      "Epoch 86/100 loss: 0.0318561335144655\n",
      "Epoch 87/100 loss: 0.03211282864823674\n",
      "Epoch 88/100 loss: 0.03190351904353783\n",
      "Epoch 89/100 loss: 0.03225077619339112\n",
      "Epoch 90/100 loss: 0.03226300468598521\n",
      "Epoch 91/100 loss: 0.031549307823714665\n",
      "Epoch 92/100 loss: 0.03166934714014342\n",
      "Epoch 93/100 loss: 0.03176330037177094\n",
      "Epoch 94/100 loss: 0.03302120925069124\n",
      "Epoch 95/100 loss: 0.032574265215993096\n",
      "Epoch 96/100 loss: 0.031700398589351156\n",
      "Epoch 97/100 loss: 0.0318231253442393\n",
      "Epoch 98/100 loss: 0.031186969152822284\n",
      "Epoch 99/100 loss: 0.03112589819467191\n",
      "Epoch 100/100 loss: 0.031017496841188156\n"
     ]
    }
   ],
   "source": [
    "losses, task_1_trained_network = train(dataloader_01, dataloader_78, 1, Network(), 100, 0.01, target_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "amber-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = test(dataloader_01_test, dataloader_78_test, 1, task_1_trained_network, target_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "scenic-burns",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.987"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "protected-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = next(iter(dataloader_01_test))\n",
    "test_inputs = test_data[0]\n",
    "test_labels = test_data[1]\n",
    "with torch.no_grad():\n",
    "    test_outputs = task_1_trained_network(test_inputs)\n",
    "mapped_test_label = np.array([label_mapping[i] for i in test_labels.numpy()])\n",
    "accuracy = sum(np.sign(test_outputs.numpy()).flatten() == mapped_test_label) / len(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "biological-store",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.987"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "radical-switch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('_layer1.weight',\n",
       "              tensor([[ 0.0113,  0.0289,  0.0323,  ..., -0.0224,  0.0070,  0.0290],\n",
       "                      [ 0.0134,  0.0121,  0.0211,  ..., -0.0042,  0.0035,  0.0199],\n",
       "                      [ 0.0299,  0.0243, -0.0155,  ..., -0.0136, -0.0132, -0.0071],\n",
       "                      ...,\n",
       "                      [-0.0077,  0.0053,  0.0162,  ..., -0.0605,  0.0052,  0.0065],\n",
       "                      [ 0.0126,  0.0111, -0.0402,  ...,  0.1217,  0.0784,  0.0037],\n",
       "                      [ 0.0242, -0.0250, -0.0070,  ...,  0.0225,  0.0309,  0.0209]])),\n",
       "             ('_layer1.bias',\n",
       "              tensor([ 2.5517, -0.0107, -0.0045, -1.3789,  1.3958,  2.4391,  1.7133,  3.4518])),\n",
       "             ('_layer2a.weight',\n",
       "              tensor([[-0.1633,  0.2989, -0.2756,  0.3890,  0.3104, -0.4713,  0.3037,  0.3088]])),\n",
       "             ('_layer2a.bias', tensor([-0.5738])),\n",
       "             ('_layer2b.weight',\n",
       "              tensor([[ 0.1236, -0.0296, -0.0863,  0.1183,  0.2382, -0.0657, -0.2177,  0.3017]])),\n",
       "             ('_layer2b.bias', tensor([0.1278]))])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_1_trained_network.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "imperial-fellowship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 loss: 0.05447167393741591\n",
      "Epoch 2/10 loss: 0.027725078008859212\n",
      "Epoch 3/10 loss: 0.023086790268217364\n",
      "Epoch 4/10 loss: 0.020417353379085526\n",
      "Epoch 5/10 loss: 0.01904322733255419\n",
      "Epoch 6/10 loss: 0.01803759840932575\n",
      "Epoch 7/10 loss: 0.017174486423052596\n",
      "Epoch 8/10 loss: 0.01696653009879623\n",
      "Epoch 9/10 loss: 0.016087961287165255\n",
      "Epoch 10/10 loss: 0.015537311530599679\n"
     ]
    }
   ],
   "source": [
    "task_1_trained_network_copy = copy.deepcopy(task_1_trained_network)\n",
    "\n",
    "task_1_trained_network_copy.switch(1)\n",
    "interpolated_dataset = get_dataset_mix(dataset_01, dataset_78, 0)\n",
    "interpolated_dataloader = torch.utils.data.DataLoader(interpolated_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "losses, task_2_trained_network = train(dataloader=interpolated_dataloader, network=task_1_trained_network_copy, epochs=10, lr=0.01, target_mapping=target_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "forty-beginning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('_layer1.weight',\n",
       "              tensor([[ 0.0113,  0.0287,  0.0321,  ..., -0.0486, -0.0087,  0.0256],\n",
       "                      [ 0.0134,  0.0121,  0.0211,  ..., -0.0042,  0.0035,  0.0199],\n",
       "                      [ 0.0299,  0.0243, -0.0155,  ...,  0.0157,  0.0146, -0.0073],\n",
       "                      ...,\n",
       "                      [-0.0077,  0.0053,  0.0162,  ..., -0.0530,  0.0109,  0.0065],\n",
       "                      [ 0.0126,  0.0111, -0.0402,  ...,  0.1195,  0.0777,  0.0037],\n",
       "                      [ 0.0242, -0.0250, -0.0070,  ...,  0.0153,  0.0285,  0.0206]])),\n",
       "             ('_layer1.bias',\n",
       "              tensor([ 2.2371, -0.0107,  1.2138, -1.4359,  1.3263,  2.9868,  2.0667,  3.4411])),\n",
       "             ('_layer2a.weight',\n",
       "              tensor([[-0.1633,  0.2989, -0.2756,  0.3890,  0.3104, -0.4713,  0.3037,  0.3088]])),\n",
       "             ('_layer2a.bias', tensor([-0.5738])),\n",
       "             ('_layer2b.weight',\n",
       "              tensor([[ 0.0358, -0.0296, -0.3671,  0.1560,  0.0246, -0.2338, -0.1217, -0.2326]])),\n",
       "             ('_layer2b.bias', tensor([0.9932]))])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_2_trained_network.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "collected-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_2_trained_network.switch(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = task_2_trained_network(test_inputs)\n",
    "    \n",
    "accuracy = sum(np.sign(test_outputs.numpy()).flatten() == mapped_test_label) / len(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "divided-overall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-provision",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-combining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Epoch 1/100 loss: 0.06868406600352739\n",
      "Epoch 2/100 loss: 0.032788023214048785\n",
      "Epoch 3/100 loss: 0.025402144108759038\n",
      "Epoch 4/100 loss: 0.02223492078672801\n",
      "Epoch 5/100 loss: 0.02059890863472718\n",
      "Epoch 6/100 loss: 0.019038617346230514\n",
      "Epoch 7/100 loss: 0.018707659077793762\n",
      "Epoch 8/100 loss: 0.017676065054317056\n",
      "Epoch 9/100 loss: 0.016790369800422207\n",
      "Epoch 10/100 loss: 0.01628994019265593\n",
      "Epoch 11/100 loss: 0.01622671794483086\n",
      "Epoch 12/100 loss: 0.015423004508470475\n",
      "Epoch 13/100 loss: 0.015322308858765897\n",
      "Epoch 14/100 loss: 0.014918374572002832\n",
      "Epoch 15/100 loss: 0.014846146110441858\n",
      "Epoch 16/100 loss: 0.014102532518473297\n",
      "Epoch 17/100 loss: 0.013735831681221111\n",
      "Epoch 18/100 loss: 0.012488326440257297\n",
      "Epoch 19/100 loss: 0.011724683211468925\n",
      "Epoch 20/100 loss: 0.011322160958655821\n",
      "Epoch 21/100 loss: 0.01115089626527794\n",
      "Epoch 22/100 loss: 0.010693621409670168\n",
      "Epoch 23/100 loss: 0.010744558653767555\n",
      "Epoch 24/100 loss: 0.010632201543020968\n",
      "Epoch 25/100 loss: 0.010273131986751243\n",
      "Epoch 26/100 loss: 0.010088530923400528\n",
      "Epoch 27/100 loss: 0.009865541394022811\n",
      "Epoch 28/100 loss: 0.010339099831926718\n",
      "Epoch 29/100 loss: 0.009735806470266825\n",
      "Epoch 30/100 loss: 0.009618257805429424\n",
      "Epoch 31/100 loss: 0.009456501470289575\n",
      "Epoch 32/100 loss: 0.009312224091716932\n",
      "Epoch 33/100 loss: 0.009563134134467504\n",
      "Epoch 34/100 loss: 0.009011834626770375\n",
      "Epoch 35/100 loss: 0.00901887912582364\n",
      "Epoch 36/100 loss: 0.008915399772773922\n",
      "Epoch 37/100 loss: 0.009114266858897675\n",
      "Epoch 38/100 loss: 0.008436912573397147\n",
      "Epoch 39/100 loss: 0.008665203017879795\n",
      "Epoch 40/100 loss: 0.00864918900826108\n",
      "Epoch 41/100 loss: 0.00850953658481233\n",
      "Epoch 42/100 loss: 0.008673455954635676\n",
      "Epoch 43/100 loss: 0.008381129488547988\n",
      "Epoch 44/100 loss: 0.008432355774786312\n",
      "Epoch 45/100 loss: 0.0082870532387335\n",
      "Epoch 46/100 loss: 0.008247688476132548\n",
      "Epoch 47/100 loss: 0.008147094037548869\n",
      "Epoch 48/100 loss: 0.008185940789255969\n",
      "Epoch 49/100 loss: 0.008039138552565009\n",
      "Epoch 50/100 loss: 0.008257154926124794\n",
      "Epoch 51/100 loss: 0.008207746905368154\n",
      "Epoch 52/100 loss: 0.007996269099846584\n",
      "Epoch 53/100 loss: 0.007824603750337429\n",
      "Epoch 54/100 loss: 0.007714441875107743\n",
      "Epoch 55/100 loss: 0.007864621497612237\n",
      "Epoch 56/100 loss: 0.0078100373437376665\n",
      "Epoch 57/100 loss: 0.00802667461148277\n",
      "Epoch 58/100 loss: 0.007912082554999894\n",
      "Epoch 59/100 loss: 0.007758652605873079\n",
      "Epoch 60/100 loss: 0.007660630599054127\n",
      "Epoch 61/100 loss: 0.0075580866414384305\n",
      "Epoch 62/100 loss: 0.007612850565471696\n",
      "Epoch 63/100 loss: 0.007677791839556704\n",
      "Epoch 64/100 loss: 0.007680162836158723\n",
      "Epoch 65/100 loss: 0.007480757661842501\n",
      "Epoch 66/100 loss: 0.007404996397613473\n",
      "Epoch 67/100 loss: 0.007404278521139903\n",
      "Epoch 68/100 loss: 0.007497438848486495\n",
      "Epoch 69/100 loss: 0.007520726482126337\n",
      "Epoch 70/100 loss: 0.007711738032814717\n",
      "Epoch 71/100 loss: 0.007409347858065369\n",
      "Epoch 72/100 loss: 0.007427838813629727\n",
      "Epoch 73/100 loss: 0.007275668203605355\n",
      "Epoch 74/100 loss: 0.007453316959028532\n",
      "Epoch 75/100 loss: 0.007252717246987605\n",
      "Epoch 76/100 loss: 0.007269879330977177\n",
      "Epoch 77/100 loss: 0.007339202993704357\n",
      "Epoch 78/100 loss: 0.00737081186668077\n",
      "Epoch 79/100 loss: 0.007095087674019581\n",
      "Epoch 80/100 loss: 0.007228655792810338\n",
      "Epoch 81/100 loss: 0.00719162896273049\n",
      "Epoch 82/100 loss: 0.007240552547853357\n",
      "Epoch 83/100 loss: 0.007199581418355321\n",
      "Epoch 84/100 loss: 0.00721911603733302\n",
      "Epoch 85/100 loss: 0.007311481994538567\n",
      "Epoch 86/100 loss: 0.0072782208385649486\n",
      "Epoch 87/100 loss: 0.007009847836277319\n",
      "Epoch 88/100 loss: 0.007134457803544527\n",
      "Epoch 89/100 loss: 0.007067403352079414\n",
      "Epoch 90/100 loss: 0.007276743173857538\n",
      "Epoch 91/100 loss: 0.006983759190934363\n",
      "Epoch 92/100 loss: 0.0070536704613668195\n",
      "Epoch 93/100 loss: 0.0071460745141306925\n",
      "Epoch 94/100 loss: 0.007044048703179135\n",
      "Epoch 95/100 loss: 0.0070012717004837\n",
      "Epoch 96/100 loss: 0.007170882012402791\n",
      "Epoch 97/100 loss: 0.006771811465357635\n",
      "Epoch 98/100 loss: 0.00706286855233633\n",
      "Epoch 99/100 loss: 0.007126405682481599\n",
      "Epoch 100/100 loss: 0.006894123507958428\n",
      "0.05263157894736842\n",
      "Epoch 1/100 loss: 0.0729585109805201\n",
      "Epoch 2/100 loss: 0.03640908505050092\n",
      "Epoch 3/100 loss: 0.029314457801758256\n",
      "Epoch 4/100 loss: 0.02594844847121367\n",
      "Epoch 5/100 loss: 0.02438387391871386\n",
      "Epoch 6/100 loss: 0.0231647999730539\n",
      "Epoch 7/100 loss: 0.021827683147637715\n",
      "Epoch 8/100 loss: 0.021384625287796007\n",
      "Epoch 9/100 loss: 0.02052120515664725\n",
      "Epoch 10/100 loss: 0.019687436807122708\n",
      "Epoch 11/100 loss: 0.019408853652597563\n",
      "Epoch 12/100 loss: 0.018623164244348435\n",
      "Epoch 13/100 loss: 0.019008965666244472\n",
      "Epoch 14/100 loss: 0.01801327097569505\n",
      "Epoch 15/100 loss: 0.016823710471220853\n",
      "Epoch 16/100 loss: 0.017054696960452597\n",
      "Epoch 17/100 loss: 0.016494839829777726\n",
      "Epoch 18/100 loss: 0.01614991321589854\n",
      "Epoch 19/100 loss: 0.016193421942572016\n",
      "Epoch 20/100 loss: 0.01595523004105429\n",
      "Epoch 21/100 loss: 0.015363193611537974\n",
      "Epoch 22/100 loss: 0.015189197272152302\n",
      "Epoch 23/100 loss: 0.014624139798393483\n",
      "Epoch 24/100 loss: 0.014759285966884868\n",
      "Epoch 25/100 loss: 0.014297079735702095\n",
      "Epoch 26/100 loss: 0.013885030503336887\n",
      "Epoch 27/100 loss: 0.013562491279859016\n",
      "Epoch 28/100 loss: 0.013163284876296017\n",
      "Epoch 29/100 loss: 0.015004454299721694\n",
      "Epoch 30/100 loss: 0.012628475345869826\n",
      "Epoch 31/100 loss: 0.012728761920910875\n",
      "Epoch 32/100 loss: 0.013147350398423276\n",
      "Epoch 33/100 loss: 0.011749801945227878\n",
      "Epoch 34/100 loss: 0.013711984192073954\n",
      "Epoch 35/100 loss: 0.013192292448586973\n",
      "Epoch 36/100 loss: 0.01291033458316058\n",
      "Epoch 37/100 loss: 0.01229114175736422\n",
      "Epoch 38/100 loss: 0.01169454664204034\n",
      "Epoch 39/100 loss: 0.012097770137134598\n",
      "Epoch 40/100 loss: 0.011723905341249709\n",
      "Epoch 41/100 loss: 0.011711903389863295\n",
      "Epoch 42/100 loss: 0.011295400080024416\n",
      "Epoch 43/100 loss: 0.012228721716223898\n",
      "Epoch 44/100 loss: 0.011317072786845728\n",
      "Epoch 45/100 loss: 0.010864000218580347\n",
      "Epoch 46/100 loss: 0.011097899044535897\n",
      "Epoch 47/100 loss: 0.010984770485277878\n",
      "Epoch 48/100 loss: 0.010729323935330399\n",
      "Epoch 49/100 loss: 0.010887693959055401\n",
      "Epoch 50/100 loss: 0.010690445321519571\n",
      "Epoch 51/100 loss: 0.010728671950190163\n",
      "Epoch 52/100 loss: 0.012080162104158394\n",
      "Epoch 53/100 loss: 0.011073110737737972\n",
      "Epoch 54/100 loss: 0.011070702132559099\n",
      "Epoch 55/100 loss: 0.01028795433443382\n",
      "Epoch 56/100 loss: 0.011604688336652515\n",
      "Epoch 57/100 loss: 0.010930557057316742\n",
      "Epoch 58/100 loss: 0.010541366531887291\n",
      "Epoch 59/100 loss: 0.010051166652140389\n",
      "Epoch 60/100 loss: 0.010521983965920887\n",
      "Epoch 61/100 loss: 0.010706461066068282\n",
      "Epoch 62/100 loss: 0.01022697532161959\n",
      "Epoch 63/100 loss: 0.010365264295854918\n",
      "Epoch 64/100 loss: 0.010324324400326238\n",
      "Epoch 65/100 loss: 0.01023738325980808\n",
      "Epoch 66/100 loss: 0.01017676695285225\n",
      "Epoch 67/100 loss: 0.00998494509981797\n",
      "Epoch 68/100 loss: 0.010213108804310953\n",
      "Epoch 69/100 loss: 0.00972376497121874\n",
      "Epoch 70/100 loss: 0.00985586133046975\n",
      "Epoch 71/100 loss: 0.009407772768166709\n",
      "Epoch 72/100 loss: 0.009839542752953841\n",
      "Epoch 73/100 loss: 0.009342020441482118\n",
      "Epoch 74/100 loss: 0.009238208660668767\n",
      "Epoch 75/100 loss: 0.009740741415704888\n",
      "Epoch 76/100 loss: 0.008992684923848377\n",
      "Epoch 77/100 loss: 0.009288190250241168\n",
      "Epoch 78/100 loss: 0.009176671222377924\n",
      "Epoch 79/100 loss: 0.010328759455207556\n",
      "Epoch 80/100 loss: 0.010023050516388736\n",
      "Epoch 81/100 loss: 0.009311199944355317\n",
      "Epoch 82/100 loss: 0.009458772419280252\n",
      "Epoch 83/100 loss: 0.009185744868484415\n",
      "Epoch 84/100 loss: 0.009057051927514157\n",
      "Epoch 85/100 loss: 0.008626976959434858\n",
      "Epoch 86/100 loss: 0.009375106345539197\n",
      "Epoch 87/100 loss: 0.008874274630776748\n",
      "Epoch 88/100 loss: 0.008901813355239315\n",
      "Epoch 89/100 loss: 0.008758439042953346\n",
      "Epoch 90/100 loss: 0.00930891693347986\n",
      "Epoch 91/100 loss: 0.008923515837214782\n",
      "Epoch 92/100 loss: 0.008462812998064388\n",
      "Epoch 93/100 loss: 0.008888771212631713\n",
      "Epoch 94/100 loss: 0.008670696771191725\n",
      "Epoch 95/100 loss: 0.009142884300616112\n",
      "Epoch 96/100 loss: 0.009003663527775615\n",
      "Epoch 97/100 loss: 0.0085241288211547\n",
      "Epoch 98/100 loss: 0.008197648964747103\n",
      "Epoch 99/100 loss: 0.008594706173450576\n",
      "Epoch 100/100 loss: 0.008446521126529655\n",
      "0.10526315789473684\n",
      "Epoch 1/100 loss: 0.07178888365828112\n",
      "Epoch 2/100 loss: 0.04080153752300028\n",
      "Epoch 3/100 loss: 0.034115790105104606\n",
      "Epoch 4/100 loss: 0.030408760976175824\n",
      "Epoch 5/100 loss: 0.028439942805865228\n",
      "Epoch 6/100 loss: 0.028090589229659613\n",
      "Epoch 7/100 loss: 0.0268479600168832\n",
      "Epoch 8/100 loss: 0.026289241071439377\n",
      "Epoch 9/100 loss: 0.02470086986716998\n",
      "Epoch 10/100 loss: 0.023494663310695267\n",
      "Epoch 11/100 loss: 0.022264295284136936\n",
      "Epoch 12/100 loss: 0.020781780626846873\n",
      "Epoch 13/100 loss: 0.020252045217049205\n",
      "Epoch 14/100 loss: 0.02013933987551099\n",
      "Epoch 15/100 loss: 0.018577062699207003\n",
      "Epoch 16/100 loss: 0.018819595169126194\n",
      "Epoch 17/100 loss: 0.019456252018192195\n",
      "Epoch 18/100 loss: 0.017912601415061274\n",
      "Epoch 19/100 loss: 0.01721451955067009\n",
      "Epoch 20/100 loss: 0.01768938891259522\n",
      "Epoch 21/100 loss: 0.01733825321811989\n",
      "Epoch 22/100 loss: 0.016606508206313517\n",
      "Epoch 23/100 loss: 0.01557390654125343\n",
      "Epoch 24/100 loss: 0.016673211439118277\n",
      "Epoch 25/100 loss: 0.015535462700936224\n",
      "Epoch 26/100 loss: 0.015411014825659597\n",
      "Epoch 27/100 loss: 0.015540815724097944\n",
      "Epoch 28/100 loss: 0.014611745281109086\n",
      "Epoch 29/100 loss: 0.01512804563968539\n",
      "Epoch 30/100 loss: 0.015188462186047929\n",
      "Epoch 31/100 loss: 0.015013917181564859\n",
      "Epoch 32/100 loss: 0.013797563985744164\n",
      "Epoch 33/100 loss: 0.014710236585203446\n",
      "Epoch 34/100 loss: 0.014407380591691539\n",
      "Epoch 35/100 loss: 0.014387920689188647\n",
      "Epoch 36/100 loss: 0.013480098814868564\n",
      "Epoch 37/100 loss: 0.013806442873105458\n",
      "Epoch 38/100 loss: 0.015391124274565756\n",
      "Epoch 39/100 loss: 0.013786413417419832\n",
      "Epoch 40/100 loss: 0.012404828385947378\n",
      "Epoch 41/100 loss: 0.012196447907129458\n",
      "Epoch 42/100 loss: 0.013078431024206381\n",
      "Epoch 43/100 loss: 0.012300663781129217\n",
      "Epoch 44/100 loss: 0.011756510153282617\n",
      "Epoch 45/100 loss: 0.012208705856734687\n",
      "Epoch 46/100 loss: 0.0122554905674258\n",
      "Epoch 47/100 loss: 0.010734403583412168\n",
      "Epoch 48/100 loss: 0.012138019645394673\n",
      "Epoch 49/100 loss: 0.012981112959676973\n",
      "Epoch 50/100 loss: 0.011455466811052369\n",
      "Epoch 51/100 loss: 0.01098727854914273\n",
      "Epoch 52/100 loss: 0.010591367170832405\n",
      "Epoch 53/100 loss: 0.011070559394941306\n",
      "Epoch 54/100 loss: 0.010407302444935027\n",
      "Epoch 55/100 loss: 0.01030628359710276\n",
      "Epoch 56/100 loss: 0.01210603735019068\n",
      "Epoch 57/100 loss: 0.010081951995447707\n",
      "Epoch 58/100 loss: 0.01059519485294688\n",
      "Epoch 59/100 loss: 0.010600912930818602\n",
      "Epoch 60/100 loss: 0.009858544095582086\n",
      "Epoch 61/100 loss: 0.0099634708017423\n",
      "Epoch 62/100 loss: 0.010088374492026824\n",
      "Epoch 63/100 loss: 0.010442845773194655\n",
      "Epoch 64/100 loss: 0.00943938865383906\n",
      "Epoch 65/100 loss: 0.010083354662139537\n",
      "Epoch 66/100 loss: 0.009294313768035495\n",
      "Epoch 67/100 loss: 0.010241539840290376\n",
      "Epoch 68/100 loss: 0.009093678645508368\n",
      "Epoch 69/100 loss: 0.009973325814191008\n",
      "Epoch 70/100 loss: 0.009663804076277819\n",
      "Epoch 71/100 loss: 0.010282380562469355\n",
      "Epoch 72/100 loss: 0.009159089481289816\n",
      "Epoch 73/100 loss: 0.009107059726223993\n",
      "Epoch 74/100 loss: 0.010579112625143666\n",
      "Epoch 75/100 loss: 0.009043219023154778\n",
      "Epoch 76/100 loss: 0.009418648166364011\n",
      "Epoch 77/100 loss: 0.009073015688601499\n",
      "Epoch 78/100 loss: 0.008914846539091326\n",
      "Epoch 79/100 loss: 0.008226168643911303\n",
      "Epoch 80/100 loss: 0.009290619454677799\n",
      "Epoch 81/100 loss: 0.008659157120709087\n",
      "Epoch 82/100 loss: 0.010063332856767743\n",
      "Epoch 83/100 loss: 0.008418494738237584\n",
      "Epoch 84/100 loss: 0.008211177763815167\n",
      "Epoch 85/100 loss: 0.006830585371625274\n",
      "Epoch 86/100 loss: 0.007394505633401235\n",
      "Epoch 87/100 loss: 0.007644178003493263\n",
      "Epoch 88/100 loss: 0.008695023577867078\n",
      "Epoch 89/100 loss: 0.00787529419108057\n",
      "Epoch 90/100 loss: 0.007735002421302557\n",
      "Epoch 91/100 loss: 0.0071126459367734695\n",
      "Epoch 92/100 loss: 0.006800350919342821\n",
      "Epoch 93/100 loss: 0.008080251638320926\n",
      "Epoch 94/100 loss: 0.007326363100872475\n",
      "Epoch 95/100 loss: 0.006683777741140946\n",
      "Epoch 96/100 loss: 0.005949240872895301\n",
      "Epoch 97/100 loss: 0.0065877489697157695\n",
      "Epoch 98/100 loss: 0.007156867204630392\n",
      "Epoch 99/100 loss: 0.00667485165709995\n",
      "Epoch 100/100 loss: 0.006555382410343385\n",
      "0.15789473684210525\n",
      "Epoch 1/100 loss: 0.07186095803128456\n",
      "Epoch 2/100 loss: 0.04510366802027007\n",
      "Epoch 3/100 loss: 0.04002342051309481\n",
      "Epoch 4/100 loss: 0.03501758816449213\n",
      "Epoch 5/100 loss: 0.0329691113798389\n",
      "Epoch 6/100 loss: 0.030490420669568773\n",
      "Epoch 7/100 loss: 0.02908976795735232\n",
      "Epoch 8/100 loss: 0.028043202992221183\n",
      "Epoch 9/100 loss: 0.0272983784901505\n",
      "Epoch 10/100 loss: 0.025361313928430603\n",
      "Epoch 11/100 loss: 0.02518859926777216\n",
      "Epoch 12/100 loss: 0.024803035731693877\n",
      "Epoch 13/100 loss: 0.024321137851807284\n",
      "Epoch 14/100 loss: 0.02417345046314015\n",
      "Epoch 15/100 loss: 0.02379078070110145\n",
      "Epoch 16/100 loss: 0.023362401329647408\n",
      "Epoch 17/100 loss: 0.023498387110529085\n",
      "Epoch 18/100 loss: 0.023141050188150217\n",
      "Epoch 19/100 loss: 0.02277138901530276\n",
      "Epoch 20/100 loss: 0.023139334736550443\n",
      "Epoch 21/100 loss: 0.02171213501595637\n",
      "Epoch 22/100 loss: 0.02216780761847263\n",
      "Epoch 23/100 loss: 0.02257308731249269\n",
      "Epoch 24/100 loss: 0.022080823706035257\n",
      "Epoch 25/100 loss: 0.021758050477933638\n",
      "Epoch 26/100 loss: 0.021841563382488015\n",
      "Epoch 27/100 loss: 0.02138862120731854\n",
      "Epoch 28/100 loss: 0.02109462040312929\n",
      "Epoch 29/100 loss: 0.02074770188265607\n",
      "Epoch 30/100 loss: 0.021830807345362115\n",
      "Epoch 31/100 loss: 0.021138937822362626\n",
      "Epoch 32/100 loss: 0.02108845959855905\n",
      "Epoch 33/100 loss: 0.02029082096561212\n",
      "Epoch 34/100 loss: 0.01981846354530002\n",
      "Epoch 35/100 loss: 0.020313047110265624\n",
      "Epoch 36/100 loss: 0.02027275542219356\n",
      "Epoch 37/100 loss: 0.020763918143189647\n",
      "Epoch 38/100 loss: 0.01981910169108454\n",
      "Epoch 39/100 loss: 0.020506796846102262\n",
      "Epoch 40/100 loss: 0.020368923594185125\n",
      "Epoch 41/100 loss: 0.020139517080833662\n",
      "Epoch 42/100 loss: 0.01994033330842124\n",
      "Epoch 43/100 loss: 0.019308649298517028\n",
      "Epoch 44/100 loss: 0.01974836995655255\n",
      "Epoch 45/100 loss: 0.019146995874364664\n",
      "Epoch 46/100 loss: 0.019153109026865175\n",
      "Epoch 47/100 loss: 0.019543549971829215\n",
      "Epoch 48/100 loss: 0.019430527539353454\n",
      "Epoch 49/100 loss: 0.01936196046376466\n",
      "Epoch 50/100 loss: 0.018946704455626764\n",
      "Epoch 51/100 loss: 0.01847062305037855\n",
      "Epoch 52/100 loss: 0.019127321234439842\n",
      "Epoch 53/100 loss: 0.018698998323793296\n",
      "Epoch 54/100 loss: 0.018212919636414374\n",
      "Epoch 55/100 loss: 0.018627434668570343\n",
      "Epoch 56/100 loss: 0.018236545661573195\n",
      "Epoch 57/100 loss: 0.01850763307507845\n",
      "Epoch 58/100 loss: 0.019115148341980866\n",
      "Epoch 59/100 loss: 0.018844643408556887\n",
      "Epoch 60/100 loss: 0.018294048687613404\n",
      "Epoch 61/100 loss: 0.01836595210392887\n",
      "Epoch 62/100 loss: 0.018315022348079853\n",
      "Epoch 63/100 loss: 0.018289925869636757\n",
      "Epoch 64/100 loss: 0.01773730387438242\n",
      "Epoch 65/100 loss: 0.017169893287353702\n",
      "Epoch 66/100 loss: 0.017814154634859358\n",
      "Epoch 67/100 loss: 0.01762742256036684\n",
      "Epoch 68/100 loss: 0.017591522571753837\n",
      "Epoch 69/100 loss: 0.01733458206142975\n",
      "Epoch 70/100 loss: 0.01719757996619594\n",
      "Epoch 71/100 loss: 0.01902620487864328\n",
      "Epoch 72/100 loss: 0.018291033004932462\n",
      "Epoch 73/100 loss: 0.017344215146326888\n",
      "Epoch 74/100 loss: 0.017081610250596693\n",
      "Epoch 75/100 loss: 0.01731873266853447\n",
      "Epoch 76/100 loss: 0.01687748557019431\n",
      "Epoch 77/100 loss: 0.016973632988019696\n",
      "Epoch 78/100 loss: 0.01645033038311214\n",
      "Epoch 79/100 loss: 0.016963000153231704\n",
      "Epoch 80/100 loss: 0.01652486152683225\n",
      "Epoch 81/100 loss: 0.017317149729195378\n",
      "Epoch 82/100 loss: 0.0167317382969803\n",
      "Epoch 83/100 loss: 0.016584999755882526\n",
      "Epoch 84/100 loss: 0.017307145562459835\n",
      "Epoch 85/100 loss: 0.01640984728364818\n",
      "Epoch 86/100 loss: 0.017013377464965018\n",
      "Epoch 87/100 loss: 0.016510564681401368\n",
      "Epoch 88/100 loss: 0.01711836922405903\n",
      "Epoch 89/100 loss: 0.01652554435158748\n",
      "Epoch 90/100 loss: 0.016692598547954737\n",
      "Epoch 91/100 loss: 0.016048009651032327\n",
      "Epoch 92/100 loss: 0.01631141839653977\n",
      "Epoch 93/100 loss: 0.016790833647070128\n",
      "Epoch 94/100 loss: 0.016366935992877387\n",
      "Epoch 95/100 loss: 0.016661886761829336\n",
      "Epoch 96/100 loss: 0.016031341327619585\n",
      "Epoch 97/100 loss: 0.01624812818684172\n",
      "Epoch 98/100 loss: 0.016145353160959653\n",
      "Epoch 99/100 loss: 0.015567388485947789\n",
      "Epoch 100/100 loss: 0.01657255115767662\n",
      "0.21052631578947367\n",
      "Epoch 1/100 loss: 0.08504398537879848\n",
      "Epoch 2/100 loss: 0.05290079499219979\n",
      "Epoch 3/100 loss: 0.0462277113521097\n",
      "Epoch 4/100 loss: 0.04374650852630593\n",
      "Epoch 5/100 loss: 0.04149664926481492\n",
      "Epoch 6/100 loss: 0.0385294614291832\n",
      "Epoch 7/100 loss: 0.038259307306696494\n",
      "Epoch 8/100 loss: 0.03896021717374778\n",
      "Epoch 9/100 loss: 0.037550944729466115\n",
      "Epoch 10/100 loss: 0.03777370106960553\n",
      "Epoch 11/100 loss: 0.0367724140679469\n",
      "Epoch 12/100 loss: 0.036030838029283974\n",
      "Epoch 13/100 loss: 0.03657068178350828\n",
      "Epoch 14/100 loss: 0.03542783749792565\n",
      "Epoch 15/100 loss: 0.035696892372405364\n",
      "Epoch 16/100 loss: 0.036289638466240975\n",
      "Epoch 17/100 loss: 0.03577205265019555\n",
      "Epoch 18/100 loss: 0.03500559103074398\n",
      "Epoch 19/100 loss: 0.035184233760427425\n",
      "Epoch 20/100 loss: 0.03529597879267032\n",
      "Epoch 21/100 loss: 0.035343998705941154\n",
      "Epoch 22/100 loss: 0.03450438439161265\n",
      "Epoch 23/100 loss: 0.03344486009517186\n",
      "Epoch 24/100 loss: 0.03349474147001547\n",
      "Epoch 25/100 loss: 0.0340164994878876\n",
      "Epoch 26/100 loss: 0.03351421283152133\n",
      "Epoch 27/100 loss: 0.03346023441400348\n",
      "Epoch 28/100 loss: 0.032095914109605245\n",
      "Epoch 29/100 loss: 0.032652183172127834\n",
      "Epoch 30/100 loss: 0.03150976540163579\n",
      "Epoch 31/100 loss: 0.03220802404248418\n",
      "Epoch 32/100 loss: 0.03159650779363617\n",
      "Epoch 33/100 loss: 0.031828951663569326\n",
      "Epoch 34/100 loss: 0.03197771819918609\n",
      "Epoch 35/100 loss: 0.03197102604376187\n",
      "Epoch 36/100 loss: 0.030884260045406\n",
      "Epoch 37/100 loss: 0.03278836721156452\n",
      "Epoch 38/100 loss: 0.03110359709041889\n",
      "Epoch 39/100 loss: 0.03182473153202207\n",
      "Epoch 40/100 loss: 0.030871199578051744\n",
      "Epoch 41/100 loss: 0.03133027511371204\n",
      "Epoch 42/100 loss: 0.031515295533424366\n",
      "Epoch 43/100 loss: 0.03108329541374086\n",
      "Epoch 44/100 loss: 0.03168967764652134\n",
      "Epoch 45/100 loss: 0.030730296615211147\n",
      "Epoch 46/100 loss: 0.03147321268164509\n",
      "Epoch 47/100 loss: 0.03152523745437348\n",
      "Epoch 48/100 loss: 0.029630767738445092\n",
      "Epoch 49/100 loss: 0.030859326448195758\n",
      "Epoch 50/100 loss: 0.029973634031399007\n",
      "Epoch 51/100 loss: 0.03045064202418219\n",
      "Epoch 52/100 loss: 0.029881019442397332\n",
      "Epoch 53/100 loss: 0.028952399723770592\n",
      "Epoch 54/100 loss: 0.032508508980949674\n",
      "Epoch 55/100 loss: 0.03268190802412609\n",
      "Epoch 56/100 loss: 0.03080761824324175\n",
      "Epoch 57/100 loss: 0.02939020733545334\n",
      "Epoch 58/100 loss: 0.029653797997360915\n",
      "Epoch 59/100 loss: 0.03044667379523436\n",
      "Epoch 60/100 loss: 0.030194292299651194\n",
      "Epoch 61/100 loss: 0.03079755887156768\n",
      "Epoch 62/100 loss: 0.02953934233856367\n",
      "Epoch 63/100 loss: 0.02976384360366018\n",
      "Epoch 64/100 loss: 0.02965170014372631\n",
      "Epoch 65/100 loss: 0.029130238595993625\n",
      "Epoch 66/100 loss: 0.030108988572915708\n",
      "Epoch 67/100 loss: 0.029262047341288257\n",
      "Epoch 68/100 loss: 0.029036419957516293\n",
      "Epoch 69/100 loss: 0.029305409383573882\n",
      "Epoch 70/100 loss: 0.030254101010960176\n",
      "Epoch 71/100 loss: 0.029371975389547715\n",
      "Epoch 72/100 loss: 0.02948118906144085\n",
      "Epoch 73/100 loss: 0.029215055013486403\n",
      "Epoch 74/100 loss: 0.028033141500507692\n",
      "Epoch 75/100 loss: 0.026973915155142882\n",
      "Epoch 76/100 loss: 0.02850780729305426\n",
      "Epoch 77/100 loss: 0.02902480243582023\n",
      "Epoch 78/100 loss: 0.029140645708723534\n",
      "Epoch 79/100 loss: 0.02922038248213646\n",
      "Epoch 80/100 loss: 0.02845428740729895\n",
      "Epoch 81/100 loss: 0.028361679135473016\n",
      "Epoch 82/100 loss: 0.02817421536193414\n",
      "Epoch 83/100 loss: 0.028130906362933958\n",
      "Epoch 84/100 loss: 0.028370513128012698\n",
      "Epoch 85/100 loss: 0.027268214434352893\n",
      "Epoch 86/100 loss: 0.027562563944589034\n",
      "Epoch 87/100 loss: 0.027539879392420783\n",
      "Epoch 88/100 loss: 0.027543099607816993\n",
      "Epoch 89/100 loss: 0.027691039116991476\n",
      "Epoch 90/100 loss: 0.02786935029301516\n",
      "Epoch 91/100 loss: 0.02755756870375251\n",
      "Epoch 92/100 loss: 0.02763782367232982\n",
      "Epoch 93/100 loss: 0.02705188064611293\n",
      "Epoch 94/100 loss: 0.028735702909026856\n",
      "Epoch 95/100 loss: 0.026812508908446185\n",
      "Epoch 96/100 loss: 0.027529623308649585\n",
      "Epoch 97/100 loss: 0.02788995814805398\n",
      "Epoch 98/100 loss: 0.027880338396711304\n",
      "Epoch 99/100 loss: 0.02813401650963089\n",
      "Epoch 100/100 loss: 0.02721079025133267\n",
      "0.2631578947368421\n",
      "Epoch 1/100 loss: 0.0865695195738092\n",
      "Epoch 2/100 loss: 0.05551401770235897\n",
      "Epoch 3/100 loss: 0.050318257854910675\n",
      "Epoch 4/100 loss: 0.04685793694693356\n",
      "Epoch 5/100 loss: 0.04517153139634451\n",
      "Epoch 6/100 loss: 0.04269588715238977\n",
      "Epoch 7/100 loss: 0.04222141782408169\n",
      "Epoch 8/100 loss: 0.04135666815037614\n",
      "Epoch 9/100 loss: 0.041818413103863826\n",
      "Epoch 10/100 loss: 0.04054135709797619\n",
      "Epoch 11/100 loss: 0.03884279089075864\n",
      "Epoch 12/100 loss: 0.038749483048387594\n",
      "Epoch 13/100 loss: 0.03799790609784556\n",
      "Epoch 14/100 loss: 0.038592230060044176\n",
      "Epoch 15/100 loss: 0.03711552515305725\n",
      "Epoch 16/100 loss: 0.03766358608860677\n",
      "Epoch 17/100 loss: 0.037507527611463094\n",
      "Epoch 18/100 loss: 0.03693118826911981\n",
      "Epoch 19/100 loss: 0.03647159257979691\n",
      "Epoch 20/100 loss: 0.03640050112425004\n",
      "Epoch 21/100 loss: 0.036414851313174124\n",
      "Epoch 22/100 loss: 0.03494646989860543\n",
      "Epoch 23/100 loss: 0.035719169978165155\n",
      "Epoch 24/100 loss: 0.03590513853176484\n",
      "Epoch 25/100 loss: 0.03564999962020234\n",
      "Epoch 26/100 loss: 0.03523085801758794\n",
      "Epoch 27/100 loss: 0.03483439967778573\n",
      "Epoch 28/100 loss: 0.034817133461082485\n",
      "Epoch 29/100 loss: 0.03436669325661638\n",
      "Epoch 30/100 loss: 0.034132175289003315\n",
      "Epoch 31/100 loss: 0.03342902346327494\n",
      "Epoch 32/100 loss: 0.03398686531773463\n",
      "Epoch 33/100 loss: 0.03347885264186647\n",
      "Epoch 34/100 loss: 0.03357802523137703\n",
      "Epoch 35/100 loss: 0.03312432396324878\n",
      "Epoch 36/100 loss: 0.03427447439333207\n",
      "Epoch 37/100 loss: 0.032953662398781725\n",
      "Epoch 38/100 loss: 0.03311284242104255\n",
      "Epoch 39/100 loss: 0.03323929793712491\n",
      "Epoch 40/100 loss: 0.03348133309739446\n",
      "Epoch 41/100 loss: 0.033156871277994986\n",
      "Epoch 42/100 loss: 0.032571231909307545\n",
      "Epoch 43/100 loss: 0.0334022270751697\n",
      "Epoch 44/100 loss: 0.032458762662658826\n",
      "Epoch 45/100 loss: 0.03234723849912138\n",
      "Epoch 46/100 loss: 0.03280095394725056\n",
      "Epoch 47/100 loss: 0.03220332176414871\n",
      "Epoch 48/100 loss: 0.03300095987979267\n",
      "Epoch 49/100 loss: 0.03218516826883515\n",
      "Epoch 50/100 loss: 0.03244246976131756\n",
      "Epoch 51/100 loss: 0.03217867617756366\n",
      "Epoch 52/100 loss: 0.03241559884250232\n",
      "Epoch 53/100 loss: 0.032230879395391855\n",
      "Epoch 54/100 loss: 0.03260811352245556\n",
      "Epoch 55/100 loss: 0.0315493932713129\n",
      "Epoch 56/100 loss: 0.031576549394214\n",
      "Epoch 57/100 loss: 0.032129965442746294\n",
      "Epoch 58/100 loss: 0.03263881679265269\n",
      "Epoch 59/100 loss: 0.031063532594210484\n",
      "Epoch 60/100 loss: 0.03192907328264112\n",
      "Epoch 61/100 loss: 0.03082342064674467\n",
      "Epoch 62/100 loss: 0.030845387305053186\n",
      "Epoch 63/100 loss: 0.03142323919803413\n",
      "Epoch 64/100 loss: 0.030645755155084695\n",
      "Epoch 65/100 loss: 0.03100935457019033\n",
      "Epoch 66/100 loss: 0.030105321078098417\n",
      "Epoch 67/100 loss: 0.029440370518074046\n",
      "Epoch 68/100 loss: 0.029936233912107618\n",
      "Epoch 69/100 loss: 0.029231340849329074\n",
      "Epoch 70/100 loss: 0.029055729370454572\n",
      "Epoch 71/100 loss: 0.028765150544887527\n",
      "Epoch 72/100 loss: 0.028694788324210768\n",
      "Epoch 73/100 loss: 0.028959023971345782\n",
      "Epoch 74/100 loss: 0.02707100875142796\n",
      "Epoch 75/100 loss: 0.02741414730151279\n",
      "Epoch 76/100 loss: 0.02752374886477432\n",
      "Epoch 77/100 loss: 0.027211910920703512\n",
      "Epoch 78/100 loss: 0.026809604362687756\n",
      "Epoch 79/100 loss: 0.025212526319308648\n",
      "Epoch 80/100 loss: 0.025751392382197886\n",
      "Epoch 81/100 loss: 0.024405778281122403\n",
      "Epoch 82/100 loss: 0.024742332045043593\n",
      "Epoch 83/100 loss: 0.02553025627389061\n",
      "Epoch 84/100 loss: 0.02418091103058695\n",
      "Epoch 85/100 loss: 0.023784145358510783\n",
      "Epoch 86/100 loss: 0.023176274212736166\n",
      "Epoch 87/100 loss: 0.02419763727432173\n",
      "Epoch 88/100 loss: 0.022177015968323333\n",
      "Epoch 89/100 loss: 0.02278892388782078\n",
      "Epoch 90/100 loss: 0.022876462434244398\n",
      "Epoch 91/100 loss: 0.02224611687199166\n",
      "Epoch 92/100 loss: 0.023033152207267326\n",
      "Epoch 93/100 loss: 0.021545666112167888\n",
      "Epoch 94/100 loss: 0.021427721314731148\n",
      "Epoch 95/100 loss: 0.02041436616403897\n",
      "Epoch 96/100 loss: 0.02026768796001456\n",
      "Epoch 97/100 loss: 0.018362406877837016\n",
      "Epoch 98/100 loss: 0.02065407034669363\n",
      "Epoch 99/100 loss: 0.021306015713905054\n",
      "Epoch 100/100 loss: 0.019261989422977792\n",
      "0.3157894736842105\n",
      "Epoch 1/100 loss: 0.1359523259728765\n",
      "Epoch 2/100 loss: 0.0696948425088362\n",
      "Epoch 3/100 loss: 0.060540429984809845\n",
      "Epoch 4/100 loss: 0.05560864215059553\n",
      "Epoch 5/100 loss: 0.05243345079464894\n",
      "Epoch 6/100 loss: 0.05172220148408657\n",
      "Epoch 7/100 loss: 0.049686568257111235\n",
      "Epoch 8/100 loss: 0.047820922290553486\n",
      "Epoch 9/100 loss: 0.04575445256616179\n",
      "Epoch 10/100 loss: 0.04191418381565549\n",
      "Epoch 11/100 loss: 0.038264992761457284\n",
      "Epoch 12/100 loss: 0.036738714766324856\n",
      "Epoch 13/100 loss: 0.03635203255960167\n",
      "Epoch 14/100 loss: 0.0337615573300276\n",
      "Epoch 15/100 loss: 0.03337695029982432\n",
      "Epoch 16/100 loss: 0.03393707876365611\n",
      "Epoch 17/100 loss: 0.03303289147267361\n",
      "Epoch 18/100 loss: 0.03216428924042065\n",
      "Epoch 19/100 loss: 0.031872434394818794\n",
      "Epoch 20/100 loss: 0.028843394641552965\n",
      "Epoch 21/100 loss: 0.03007969254365687\n",
      "Epoch 22/100 loss: 0.030195082781784646\n",
      "Epoch 23/100 loss: 0.030003378702737248\n",
      "Epoch 24/100 loss: 0.028913949811246793\n",
      "Epoch 25/100 loss: 0.027259891791005988\n",
      "Epoch 26/100 loss: 0.026561094827514387\n",
      "Epoch 27/100 loss: 0.027313385808419098\n",
      "Epoch 28/100 loss: 0.026789373403858466\n",
      "Epoch 29/100 loss: 0.02603152264663375\n",
      "Epoch 30/100 loss: 0.026509452251905482\n",
      "Epoch 31/100 loss: 0.025322589358181772\n",
      "Epoch 32/100 loss: 0.025914923847762134\n",
      "Epoch 33/100 loss: 0.02606621069031112\n",
      "Epoch 34/100 loss: 0.024204086762036493\n",
      "Epoch 35/100 loss: 0.02401688318711039\n",
      "Epoch 36/100 loss: 0.02471497744072427\n",
      "Epoch 37/100 loss: 0.026286254981295793\n",
      "Epoch 38/100 loss: 0.024666739813029116\n",
      "Epoch 39/100 loss: 0.024055735898389358\n",
      "Epoch 40/100 loss: 0.02332712539409536\n",
      "Epoch 41/100 loss: 0.024871788085025714\n",
      "Epoch 42/100 loss: 0.02476819392203329\n",
      "Epoch 43/100 loss: 0.024485937455934515\n",
      "Epoch 44/100 loss: 0.023406274948220878\n",
      "Epoch 45/100 loss: 0.024485559545321545\n",
      "Epoch 46/100 loss: 0.024111114833100997\n",
      "Epoch 47/100 loss: 0.024052356714434276\n",
      "Epoch 48/100 loss: 0.023853105319177694\n",
      "Epoch 49/100 loss: 0.022670824976009356\n",
      "Epoch 50/100 loss: 0.02192162948781725\n",
      "Epoch 51/100 loss: 0.022446401603029822\n",
      "Epoch 52/100 loss: 0.02338526688494503\n",
      "Epoch 53/100 loss: 0.02384203237135497\n",
      "Epoch 54/100 loss: 0.022329198631047605\n",
      "Epoch 55/100 loss: 0.023733912547741567\n",
      "Epoch 56/100 loss: 0.021807919268448175\n",
      "Epoch 57/100 loss: 0.020233154287362845\n",
      "Epoch 58/100 loss: 0.022029390712557387\n",
      "Epoch 59/100 loss: 0.022039721850172365\n",
      "Epoch 60/100 loss: 0.023066897272866153\n",
      "Epoch 61/100 loss: 0.019502226913280893\n",
      "Epoch 62/100 loss: 0.02226764085348085\n",
      "Epoch 63/100 loss: 0.02447511523172226\n",
      "Epoch 64/100 loss: 0.02208303700781156\n",
      "Epoch 65/100 loss: 0.020491628320818873\n",
      "Epoch 66/100 loss: 0.021185423390656977\n",
      "Epoch 67/100 loss: 0.021853139930993356\n",
      "Epoch 68/100 loss: 0.02120540545240977\n",
      "Epoch 69/100 loss: 0.02231764636662564\n",
      "Epoch 70/100 loss: 0.021874161462400298\n",
      "Epoch 71/100 loss: 0.02087485071250424\n",
      "Epoch 72/100 loss: 0.020776449048455096\n",
      "Epoch 73/100 loss: 0.021328102476540753\n",
      "Epoch 74/100 loss: 0.02139980493417779\n",
      "Epoch 75/100 loss: 0.0197238500780461\n",
      "Epoch 76/100 loss: 0.020210969040745045\n",
      "Epoch 77/100 loss: 0.022744404829762355\n",
      "Epoch 78/100 loss: 0.018908001082558204\n",
      "Epoch 79/100 loss: 0.02014164895620648\n",
      "Epoch 80/100 loss: 0.02106686629458423\n",
      "Epoch 81/100 loss: 0.019261995019071925\n",
      "Epoch 82/100 loss: 0.02151829223421414\n",
      "Epoch 83/100 loss: 0.01977515590835585\n",
      "Epoch 84/100 loss: 0.019344283017351725\n",
      "Epoch 85/100 loss: 0.02069531024518081\n",
      "Epoch 86/100 loss: 0.02261176434657434\n",
      "Epoch 87/100 loss: 0.024993765484657288\n",
      "Epoch 88/100 loss: 0.02018560066695873\n",
      "Epoch 89/100 loss: 0.020648706513188127\n",
      "Epoch 90/100 loss: 0.019914114603278864\n",
      "Epoch 91/100 loss: 0.02049931240430142\n",
      "Epoch 92/100 loss: 0.01804410745430649\n",
      "Epoch 93/100 loss: 0.019318803895908006\n",
      "Epoch 94/100 loss: 0.021494752159594937\n",
      "Epoch 95/100 loss: 0.020123644544523443\n",
      "Epoch 96/100 loss: 0.018040118184249214\n",
      "Epoch 97/100 loss: 0.020331364716065885\n",
      "Epoch 98/100 loss: 0.02026423698122487\n",
      "Epoch 99/100 loss: 0.018716508962197228\n",
      "Epoch 100/100 loss: 0.019693001269386983\n",
      "0.3684210526315789\n",
      "Epoch 1/100 loss: 0.09975665160164808\n",
      "Epoch 2/100 loss: 0.0723421763653305\n",
      "Epoch 3/100 loss: 0.06594626582298503\n",
      "Epoch 4/100 loss: 0.061957152011829386\n",
      "Epoch 5/100 loss: 0.05891263135696227\n",
      "Epoch 6/100 loss: 0.055529879081425274\n",
      "Epoch 7/100 loss: 0.05513046506728676\n",
      "Epoch 8/100 loss: 0.051421200042753565\n",
      "Epoch 9/100 loss: 0.04959847359102369\n",
      "Epoch 10/100 loss: 0.047835450266494474\n",
      "Epoch 11/100 loss: 0.046870120005166035\n",
      "Epoch 12/100 loss: 0.046319457692749914\n",
      "Epoch 13/100 loss: 0.04537394861142024\n",
      "Epoch 14/100 loss: 0.04391611645698676\n",
      "Epoch 15/100 loss: 0.045095366331806656\n",
      "Epoch 16/100 loss: 0.04308040005035157\n",
      "Epoch 17/100 loss: 0.04285919412730457\n",
      "Epoch 18/100 loss: 0.042581511298755134\n",
      "Epoch 19/100 loss: 0.04277447882440714\n",
      "Epoch 20/100 loss: 0.0415142430361539\n",
      "Epoch 21/100 loss: 0.041802740826774744\n",
      "Epoch 22/100 loss: 0.0408758970801641\n",
      "Epoch 23/100 loss: 0.041190224325164705\n",
      "Epoch 24/100 loss: 0.03990832228738461\n",
      "Epoch 25/100 loss: 0.0404019291138846\n",
      "Epoch 26/100 loss: 0.03955921820768689\n",
      "Epoch 27/100 loss: 0.039964758716533266\n",
      "Epoch 28/100 loss: 0.040280357024187786\n",
      "Epoch 29/100 loss: 0.03791380770557718\n",
      "Epoch 30/100 loss: 0.03943821680883538\n",
      "Epoch 31/100 loss: 0.038368232342985274\n",
      "Epoch 32/100 loss: 0.038361888808565836\n",
      "Epoch 33/100 loss: 0.03812075580006451\n",
      "Epoch 34/100 loss: 0.03821759788624849\n",
      "Epoch 35/100 loss: 0.03849954701350277\n",
      "Epoch 36/100 loss: 0.03741936980901281\n",
      "Epoch 37/100 loss: 0.038052435255893136\n",
      "Epoch 38/100 loss: 0.03761310608729855\n",
      "Epoch 39/100 loss: 0.03811312932167848\n",
      "Epoch 40/100 loss: 0.03658817966101939\n",
      "Epoch 41/100 loss: 0.03757637729392045\n",
      "Epoch 42/100 loss: 0.03691367521546752\n",
      "Epoch 43/100 loss: 0.03654364989406684\n",
      "Epoch 44/100 loss: 0.03619452044882282\n",
      "Epoch 45/100 loss: 0.03671492074329722\n",
      "Epoch 46/100 loss: 0.03790427322706817\n",
      "Epoch 47/100 loss: 0.036005866831294864\n",
      "Epoch 48/100 loss: 0.036289362101661414\n",
      "Epoch 49/100 loss: 0.03720443536711711\n",
      "Epoch 50/100 loss: 0.03747612070679123\n",
      "Epoch 51/100 loss: 0.036756935172059337\n",
      "Epoch 52/100 loss: 0.03739825009426219\n",
      "Epoch 53/100 loss: 0.035485583579819506\n",
      "Epoch 54/100 loss: 0.03583948706371453\n",
      "Epoch 55/100 loss: 0.033326019443778\n",
      "Epoch 56/100 loss: 0.03416594364585456\n",
      "Epoch 57/100 loss: 0.03417395523547043\n",
      "Epoch 58/100 loss: 0.035516202370257814\n",
      "Epoch 59/100 loss: 0.03535204724711623\n",
      "Epoch 60/100 loss: 0.03543827012750652\n",
      "Epoch 61/100 loss: 0.03479620781062758\n",
      "Epoch 62/100 loss: 0.03549784948609326\n",
      "Epoch 63/100 loss: 0.033564503762017084\n",
      "Epoch 64/100 loss: 0.03491829897781428\n",
      "Epoch 65/100 loss: 0.03375684284989775\n",
      "Epoch 66/100 loss: 0.035026445981141664\n",
      "Epoch 67/100 loss: 0.033827887285224754\n",
      "Epoch 68/100 loss: 0.03441569828977061\n",
      "Epoch 69/100 loss: 0.03504456672069435\n",
      "Epoch 70/100 loss: 0.033969161943436445\n",
      "Epoch 71/100 loss: 0.03508124992375111\n",
      "Epoch 72/100 loss: 0.03401071921285902\n",
      "Epoch 73/100 loss: 0.034076425847162035\n",
      "Epoch 74/100 loss: 0.034109294373517916\n",
      "Epoch 75/100 loss: 0.03215439120890396\n",
      "Epoch 76/100 loss: 0.031778004718922136\n",
      "Epoch 77/100 loss: 0.0334499716104909\n",
      "Epoch 78/100 loss: 0.03325342590932256\n",
      "Epoch 79/100 loss: 0.03247474613096341\n",
      "Epoch 80/100 loss: 0.034082744569644184\n",
      "Epoch 81/100 loss: 0.03274106923553393\n",
      "Epoch 82/100 loss: 0.03222071981996811\n",
      "Epoch 83/100 loss: 0.03232488686613047\n",
      "Epoch 84/100 loss: 0.03303868798354433\n",
      "Epoch 85/100 loss: 0.03356165813005361\n",
      "Epoch 86/100 loss: 0.03270698713367731\n",
      "Epoch 87/100 loss: 0.033399373091002674\n",
      "Epoch 88/100 loss: 0.03367277712516237\n",
      "Epoch 89/100 loss: 0.03150498956656417\n",
      "Epoch 90/100 loss: 0.03278065234828387\n",
      "Epoch 91/100 loss: 0.03239205490897726\n",
      "Epoch 92/100 loss: 0.03221003975393456\n",
      "Epoch 93/100 loss: 0.031979928449789766\n",
      "Epoch 94/100 loss: 0.03226935804210463\n",
      "Epoch 95/100 loss: 0.032175904245348594\n",
      "Epoch 96/100 loss: 0.0323213511039932\n",
      "Epoch 97/100 loss: 0.03223378431013758\n",
      "Epoch 98/100 loss: 0.03188811186566263\n",
      "Epoch 99/100 loss: 0.03205265924922744\n",
      "Epoch 100/100 loss: 0.032421088507023806\n",
      "0.42105263157894735\n",
      "Epoch 1/100 loss: 0.111064597109208\n",
      "Epoch 2/100 loss: 0.07743559808420658\n",
      "Epoch 3/100 loss: 0.06690175253719258\n",
      "Epoch 4/100 loss: 0.06085873184302011\n",
      "Epoch 5/100 loss: 0.05651002838561663\n",
      "Epoch 6/100 loss: 0.054433493155160044\n",
      "Epoch 7/100 loss: 0.053127875360541206\n",
      "Epoch 8/100 loss: 0.05050232469389698\n",
      "Epoch 9/100 loss: 0.05059743117565346\n",
      "Epoch 10/100 loss: 0.04745049577949665\n",
      "Epoch 11/100 loss: 0.046163432174573735\n",
      "Epoch 12/100 loss: 0.04913572524816338\n",
      "Epoch 13/100 loss: 0.04541328902389174\n",
      "Epoch 14/100 loss: 0.04630018291612232\n",
      "Epoch 15/100 loss: 0.04668136258209353\n",
      "Epoch 16/100 loss: 0.04646258559534859\n",
      "Epoch 17/100 loss: 0.04374581595600207\n",
      "Epoch 18/100 loss: 0.04440563258532103\n",
      "Epoch 19/100 loss: 0.04337593976133864\n",
      "Epoch 20/100 loss: 0.040784096291695854\n",
      "Epoch 21/100 loss: 0.041377702871305835\n",
      "Epoch 22/100 loss: 0.0415830127558024\n",
      "Epoch 23/100 loss: 0.04416240559532506\n",
      "Epoch 24/100 loss: 0.04179526945126487\n",
      "Epoch 25/100 loss: 0.041374997389071445\n",
      "Epoch 26/100 loss: 0.04351666697661583\n",
      "Epoch 27/100 loss: 0.04000024282399316\n",
      "Epoch 28/100 loss: 0.04037376895321688\n",
      "Epoch 29/100 loss: 0.04175872305155345\n",
      "Epoch 30/100 loss: 0.040558298150702776\n",
      "Epoch 31/100 loss: 0.03991353082266943\n",
      "Epoch 32/100 loss: 0.0409964291578258\n",
      "Epoch 33/100 loss: 0.041314796906821036\n",
      "Epoch 34/100 loss: 0.039827744537052547\n",
      "Epoch 35/100 loss: 0.04208314047106354\n",
      "Epoch 36/100 loss: 0.041170936643491404\n",
      "Epoch 37/100 loss: 0.03805866528015294\n",
      "Epoch 38/100 loss: 0.0396389172965367\n",
      "Epoch 39/100 loss: 0.04080287299111529\n",
      "Epoch 40/100 loss: 0.03836780567528614\n",
      "Epoch 41/100 loss: 0.04105671103557639\n",
      "Epoch 42/100 loss: 0.03925751193740136\n",
      "Epoch 43/100 loss: 0.03837230951766422\n",
      "Epoch 44/100 loss: 0.03697160408786372\n",
      "Epoch 45/100 loss: 0.03852939496013699\n",
      "Epoch 46/100 loss: 0.037982586131155176\n",
      "Epoch 47/100 loss: 0.03836852308278836\n",
      "Epoch 48/100 loss: 0.03807660264743655\n",
      "Epoch 49/100 loss: 0.04180706623852301\n",
      "Epoch 50/100 loss: 0.036468193347788024\n",
      "Epoch 51/100 loss: 0.041556213887593764\n",
      "Epoch 52/100 loss: 0.03990693909499858\n",
      "Epoch 53/100 loss: 0.04050914499916608\n",
      "Epoch 54/100 loss: 0.03858936670437608\n",
      "Epoch 55/100 loss: 0.03898457135349741\n",
      "Epoch 56/100 loss: 0.03887450443869744\n",
      "Epoch 57/100 loss: 0.03760483106448388\n",
      "Epoch 58/100 loss: 0.03751774853716125\n",
      "Epoch 59/100 loss: 0.03742328783372805\n",
      "Epoch 60/100 loss: 0.03542928752584065\n",
      "Epoch 61/100 loss: 0.036570939568939476\n",
      "Epoch 62/100 loss: 0.03664077677119701\n",
      "Epoch 63/100 loss: 0.03510329745390269\n",
      "Epoch 64/100 loss: 0.03346710327437989\n",
      "Epoch 65/100 loss: 0.03270513649391991\n",
      "Epoch 66/100 loss: 0.03443033571260238\n",
      "Epoch 67/100 loss: 0.0371879152602196\n",
      "Epoch 68/100 loss: 0.03325737744790635\n",
      "Epoch 69/100 loss: 0.03415309693587773\n",
      "Epoch 70/100 loss: 0.03313698683184195\n",
      "Epoch 71/100 loss: 0.032439377079661144\n",
      "Epoch 72/100 loss: 0.03717053072557673\n",
      "Epoch 73/100 loss: 0.034460741575189774\n",
      "Epoch 74/100 loss: 0.03329786273359404\n",
      "Epoch 75/100 loss: 0.03158614757986731\n",
      "Epoch 76/100 loss: 0.03356842888883993\n",
      "Epoch 77/100 loss: 0.032536221246667024\n",
      "Epoch 78/100 loss: 0.03326907749636578\n",
      "Epoch 79/100 loss: 0.03414632305228846\n",
      "Epoch 80/100 loss: 0.03460436013011519\n",
      "Epoch 81/100 loss: 0.033857526075571666\n",
      "Epoch 82/100 loss: 0.03397002293069634\n",
      "Epoch 83/100 loss: 0.034897985380597685\n",
      "Epoch 84/100 loss: 0.03240529544580133\n",
      "Epoch 85/100 loss: 0.03348608656997842\n",
      "Epoch 86/100 loss: 0.03335409268150385\n",
      "Epoch 87/100 loss: 0.03421657996934621\n",
      "Epoch 88/100 loss: 0.03413725249351311\n",
      "Epoch 89/100 loss: 0.03243419502638398\n",
      "Epoch 90/100 loss: 0.03458955678156097\n",
      "Epoch 91/100 loss: 0.03275160369225028\n",
      "Epoch 92/100 loss: 0.03200462590265378\n",
      "Epoch 93/100 loss: 0.03320244966739222\n",
      "Epoch 94/100 loss: 0.03337648330970769\n",
      "Epoch 95/100 loss: 0.03215249049591741\n",
      "Epoch 96/100 loss: 0.03153129086898378\n",
      "Epoch 97/100 loss: 0.03109484933079499\n",
      "Epoch 98/100 loss: 0.03195506416308164\n",
      "Epoch 99/100 loss: 0.03402243629525866\n",
      "Epoch 100/100 loss: 0.032695145466297135\n",
      "0.47368421052631576\n",
      "Epoch 1/100 loss: 0.12651450152471672\n",
      "Epoch 2/100 loss: 0.0836496693022254\n",
      "Epoch 3/100 loss: 0.07131598729588018\n",
      "Epoch 4/100 loss: 0.06896655660729968\n",
      "Epoch 5/100 loss: 0.0649762753880728\n",
      "Epoch 6/100 loss: 0.06200209328976088\n",
      "Epoch 7/100 loss: 0.06069060393360805\n",
      "Epoch 8/100 loss: 0.061182184599897285\n",
      "Epoch 9/100 loss: 0.05971027399086427\n",
      "Epoch 10/100 loss: 0.05797538448770255\n",
      "Epoch 11/100 loss: 0.05806112575191274\n",
      "Epoch 12/100 loss: 0.05602902124804564\n",
      "Epoch 13/100 loss: 0.05692024872334893\n",
      "Epoch 14/100 loss: 0.055821358156521096\n",
      "Epoch 15/100 loss: 0.0534390691288113\n",
      "Epoch 16/100 loss: 0.05522640115962713\n",
      "Epoch 17/100 loss: 0.05281235844772208\n",
      "Epoch 18/100 loss: 0.05170363927984001\n",
      "Epoch 19/100 loss: 0.053694242278804025\n",
      "Epoch 20/100 loss: 0.05126963714706877\n",
      "Epoch 21/100 loss: 0.05107899817242755\n",
      "Epoch 22/100 loss: 0.052901642967835304\n",
      "Epoch 23/100 loss: 0.050032449923066734\n",
      "Epoch 24/100 loss: 0.05127924937993803\n",
      "Epoch 25/100 loss: 0.04962244084363131\n",
      "Epoch 26/100 loss: 0.051289045407253694\n",
      "Epoch 27/100 loss: 0.04933541893419841\n",
      "Epoch 28/100 loss: 0.049449565208418214\n",
      "Epoch 29/100 loss: 0.047990800876491636\n",
      "Epoch 30/100 loss: 0.04833841851078774\n",
      "Epoch 31/100 loss: 0.048265929031510835\n",
      "Epoch 32/100 loss: 0.04750898853679208\n",
      "Epoch 33/100 loss: 0.049331686194061225\n",
      "Epoch 34/100 loss: 0.048596686945115125\n",
      "Epoch 35/100 loss: 0.04749355131379686\n",
      "Epoch 36/100 loss: 0.0472425708150222\n",
      "Epoch 37/100 loss: 0.04814588801557766\n",
      "Epoch 38/100 loss: 0.04765700059816217\n",
      "Epoch 39/100 loss: 0.048243858384672676\n",
      "Epoch 40/100 loss: 0.047685830397866565\n",
      "Epoch 41/100 loss: 0.047670599166435276\n",
      "Epoch 42/100 loss: 0.04971117956643035\n",
      "Epoch 43/100 loss: 0.04680731949867362\n",
      "Epoch 44/100 loss: 0.04763099156147258\n",
      "Epoch 45/100 loss: 0.049278600224978285\n",
      "Epoch 46/100 loss: 0.047381518854202755\n",
      "Epoch 47/100 loss: 0.04885981781593269\n",
      "Epoch 48/100 loss: 0.04823142670942999\n",
      "Epoch 49/100 loss: 0.048016142730880916\n",
      "Epoch 50/100 loss: 0.0449256879043765\n",
      "Epoch 51/100 loss: 0.04598385865191501\n",
      "Epoch 52/100 loss: 0.045713423081874385\n",
      "Epoch 53/100 loss: 0.04640397123307726\n",
      "Epoch 54/100 loss: 0.04835127764529064\n",
      "Epoch 55/100 loss: 0.047516124878317\n",
      "Epoch 56/100 loss: 0.048884040933125913\n",
      "Epoch 57/100 loss: 0.04767964876877476\n",
      "Epoch 58/100 loss: 0.04563024820541078\n",
      "Epoch 59/100 loss: 0.04710774281547898\n",
      "Epoch 60/100 loss: 0.04445405220642485\n",
      "Epoch 61/100 loss: 0.047609766814271\n",
      "Epoch 62/100 loss: 0.04499027355525218\n",
      "Epoch 63/100 loss: 0.04426358112816354\n",
      "Epoch 64/100 loss: 0.045850769395396604\n",
      "Epoch 65/100 loss: 0.04418032203521159\n",
      "Epoch 66/100 loss: 0.045754762957355845\n",
      "Epoch 67/100 loss: 0.04709647992134548\n",
      "Epoch 68/100 loss: 0.045949828789541014\n",
      "Epoch 69/100 loss: 0.04520060961291912\n",
      "Epoch 70/100 loss: 0.04569991911204217\n",
      "Epoch 71/100 loss: 0.0454526015540347\n",
      "Epoch 72/100 loss: 0.04442369932908247\n",
      "Epoch 73/100 loss: 0.04372067110839558\n",
      "Epoch 74/100 loss: 0.046044424797203914\n",
      "Epoch 75/100 loss: 0.045622176095012075\n",
      "Epoch 76/100 loss: 0.046119133489602226\n",
      "Epoch 77/100 loss: 0.04409382277527986\n",
      "Epoch 78/100 loss: 0.04496273865350997\n",
      "Epoch 79/100 loss: 0.04422601779405468\n",
      "Epoch 80/100 loss: 0.0441835528483198\n",
      "Epoch 81/100 loss: 0.044786831497091346\n",
      "Epoch 82/100 loss: 0.0435348036338076\n",
      "Epoch 83/100 loss: 0.04762147664533359\n",
      "Epoch 84/100 loss: 0.04603269637930612\n",
      "Epoch 85/100 loss: 0.04484382338765989\n",
      "Epoch 86/100 loss: 0.043280072338773075\n",
      "Epoch 87/100 loss: 0.041341819010138156\n",
      "Epoch 88/100 loss: 0.045206189223941144\n",
      "Epoch 89/100 loss: 0.04418145256061212\n",
      "Epoch 90/100 loss: 0.04000336735020817\n",
      "Epoch 91/100 loss: 0.04429429700566877\n",
      "Epoch 92/100 loss: 0.041526765901081945\n",
      "Epoch 93/100 loss: 0.04305354357856704\n",
      "Epoch 94/100 loss: 0.041355207324974726\n",
      "Epoch 95/100 loss: 0.045350360863102464\n",
      "Epoch 96/100 loss: 0.042138085757560916\n",
      "Epoch 97/100 loss: 0.04163940331483946\n",
      "Epoch 98/100 loss: 0.042616530041204775\n",
      "Epoch 99/100 loss: 0.0427593774862946\n",
      "Epoch 100/100 loss: 0.04023737993376725\n",
      "0.5263157894736842\n",
      "Epoch 1/100 loss: 0.15784170902906228\n",
      "Epoch 2/100 loss: 0.11790450150979292\n",
      "Epoch 3/100 loss: 0.11015572090802414\n",
      "Epoch 4/100 loss: 0.1054628489221527\n",
      "Epoch 5/100 loss: 0.10208756248297264\n",
      "Epoch 6/100 loss: 0.09907733949820768\n",
      "Epoch 7/100 loss: 0.09756430361501238\n",
      "Epoch 8/100 loss: 0.0979140671578839\n",
      "Epoch 9/100 loss: 0.09592703929056658\n",
      "Epoch 10/100 loss: 0.09676786026344518\n",
      "Epoch 11/100 loss: 0.0963589959237701\n",
      "Epoch 12/100 loss: 0.09616300534170871\n",
      "Epoch 13/100 loss: 0.09553775376738086\n",
      "Epoch 14/100 loss: 0.09176976262622179\n",
      "Epoch 15/100 loss: 0.09510113098764206\n",
      "Epoch 16/100 loss: 0.09295255226749029\n",
      "Epoch 17/100 loss: 0.09617922845786085\n",
      "Epoch 18/100 loss: 0.09332748720806433\n",
      "Epoch 19/100 loss: 0.0926639551191355\n",
      "Epoch 20/100 loss: 0.09268654610087712\n",
      "Epoch 21/100 loss: 0.09339411852222666\n",
      "Epoch 22/100 loss: 0.09090785075465703\n",
      "Epoch 23/100 loss: 0.0929996418984889\n",
      "Epoch 24/100 loss: 0.09201496364366342\n",
      "Epoch 25/100 loss: 0.09397164599029238\n",
      "Epoch 26/100 loss: 0.09203935622305448\n",
      "Epoch 27/100 loss: 0.09094245095267381\n",
      "Epoch 28/100 loss: 0.09144628646727954\n",
      "Epoch 29/100 loss: 0.09101815328377105\n",
      "Epoch 30/100 loss: 0.09279209751903199\n",
      "Epoch 31/100 loss: 0.09153191108221519\n",
      "Epoch 32/100 loss: 0.08961929054818336\n",
      "Epoch 33/100 loss: 0.08873399061354441\n",
      "Epoch 34/100 loss: 0.0891536358984197\n",
      "Epoch 35/100 loss: 0.09170367066528816\n",
      "Epoch 36/100 loss: 0.09228841584365309\n",
      "Epoch 37/100 loss: 0.091792175432513\n",
      "Epoch 38/100 loss: 0.09055055523879366\n",
      "Epoch 39/100 loss: 0.09114813049445271\n",
      "Epoch 40/100 loss: 0.08887211900819063\n",
      "Epoch 41/100 loss: 0.0915722974350239\n",
      "Epoch 42/100 loss: 0.08834168298935659\n",
      "Epoch 43/100 loss: 0.08885511705197899\n",
      "Epoch 44/100 loss: 0.08971852088735413\n",
      "Epoch 45/100 loss: 0.0900684246660071\n",
      "Epoch 46/100 loss: 0.09040111702412698\n",
      "Epoch 47/100 loss: 0.0889213122097415\n",
      "Epoch 48/100 loss: 0.09081296627058076\n",
      "Epoch 49/100 loss: 0.08784995608903659\n",
      "Epoch 50/100 loss: 0.09025151881530545\n",
      "Epoch 51/100 loss: 0.08889837798849654\n",
      "Epoch 52/100 loss: 0.08920118081472082\n",
      "Epoch 53/100 loss: 0.09082614649682787\n",
      "Epoch 54/100 loss: 0.08828350032090122\n",
      "Epoch 55/100 loss: 0.0883455241139363\n",
      "Epoch 56/100 loss: 0.08810828943512415\n",
      "Epoch 57/100 loss: 0.08833429943556595\n",
      "Epoch 58/100 loss: 0.08802268112549223\n",
      "Epoch 59/100 loss: 0.09081960821156912\n",
      "Epoch 60/100 loss: 0.08999280923788694\n",
      "Epoch 61/100 loss: 0.08902059528707192\n",
      "Epoch 62/100 loss: 0.08732983332074933\n",
      "Epoch 63/100 loss: 0.08804265163539159\n",
      "Epoch 64/100 loss: 0.09167685522287568\n",
      "Epoch 65/100 loss: 0.08932927737993888\n",
      "Epoch 66/100 loss: 0.08964881131721944\n",
      "Epoch 67/100 loss: 0.08958555470879105\n",
      "Epoch 68/100 loss: 0.08843090467197778\n",
      "Epoch 69/100 loss: 0.0890126951171035\n",
      "Epoch 70/100 loss: 0.08912438599527231\n",
      "Epoch 71/100 loss: 0.08915076651460829\n",
      "Epoch 72/100 loss: 0.08753108160223504\n",
      "Epoch 73/100 loss: 0.08784561457171473\n",
      "Epoch 74/100 loss: 0.08862809556361116\n",
      "Epoch 75/100 loss: 0.08654704298874727\n",
      "Epoch 76/100 loss: 0.08746465378602382\n",
      "Epoch 77/100 loss: 0.08838205832255579\n",
      "Epoch 78/100 loss: 0.08888653759562222\n",
      "Epoch 79/100 loss: 0.08867984344512486\n",
      "Epoch 80/100 loss: 0.08852005079599047\n",
      "Epoch 81/100 loss: 0.08956892952317025\n",
      "Epoch 82/100 loss: 0.08713439488530128\n",
      "Epoch 83/100 loss: 0.08922635648462657\n",
      "Epoch 84/100 loss: 0.08785814628184871\n",
      "Epoch 85/100 loss: 0.08635457244931088\n",
      "Epoch 86/100 loss: 0.08876806575209398\n",
      "Epoch 87/100 loss: 0.08750511885978823\n",
      "Epoch 88/100 loss: 0.08902643429961064\n",
      "Epoch 89/100 loss: 0.08633732656574006\n",
      "Epoch 90/100 loss: 0.08787299797559699\n",
      "Epoch 91/100 loss: 0.08797750466991945\n",
      "Epoch 92/100 loss: 0.08883548874345983\n",
      "Epoch 93/100 loss: 0.08832188248652129\n",
      "Epoch 94/100 loss: 0.08687036726869667\n",
      "Epoch 95/100 loss: 0.08866348781808737\n",
      "Epoch 96/100 loss: 0.08730561504121633\n",
      "Epoch 97/100 loss: 0.08681411881147638\n",
      "Epoch 98/100 loss: 0.08843802814025521\n",
      "Epoch 99/100 loss: 0.08875512162030126\n",
      "Epoch 100/100 loss: 0.0880191181495569\n",
      "0.5789473684210527\n",
      "Epoch 1/100 loss: 0.14679909535734112\n",
      "Epoch 2/100 loss: 0.11127026323875983\n",
      "Epoch 3/100 loss: 0.10214210878748728\n",
      "Epoch 4/100 loss: 0.09994801348331589\n",
      "Epoch 5/100 loss: 0.09966990340009213\n",
      "Epoch 6/100 loss: 0.09509747585672755\n",
      "Epoch 7/100 loss: 0.0935426057700564\n",
      "Epoch 8/100 loss: 0.09366901522897035\n",
      "Epoch 9/100 loss: 0.092452953275069\n",
      "Epoch 10/100 loss: 0.09045460847774717\n",
      "Epoch 11/100 loss: 0.09147636289803882\n",
      "Epoch 12/100 loss: 0.08809271434771666\n",
      "Epoch 13/100 loss: 0.08926876994593473\n",
      "Epoch 14/100 loss: 0.08683776121135871\n",
      "Epoch 15/100 loss: 0.08902633741267699\n",
      "Epoch 16/100 loss: 0.08711794713427583\n",
      "Epoch 17/100 loss: 0.08810325907368657\n",
      "Epoch 18/100 loss: 0.08512106530492217\n",
      "Epoch 19/100 loss: 0.08302312920869155\n",
      "Epoch 20/100 loss: 0.08506823039477943\n",
      "Epoch 21/100 loss: 0.08231999760653122\n",
      "Epoch 22/100 loss: 0.08043710647156783\n",
      "Epoch 23/100 loss: 0.07997911408915413\n",
      "Epoch 24/100 loss: 0.07854832975936167\n",
      "Epoch 25/100 loss: 0.07855927883800602\n",
      "Epoch 26/100 loss: 0.07746113488520635\n",
      "Epoch 27/100 loss: 0.07367649137110144\n",
      "Epoch 28/100 loss: 0.07381544895704407\n",
      "Epoch 29/100 loss: 0.07531565361659233\n",
      "Epoch 30/100 loss: 0.07337792694325324\n",
      "Epoch 31/100 loss: 0.07244935655183073\n",
      "Epoch 32/100 loss: 0.07076104515899928\n",
      "Epoch 33/100 loss: 0.07071035909252966\n",
      "Epoch 34/100 loss: 0.06869971672883554\n",
      "Epoch 35/100 loss: 0.06856414565552588\n",
      "Epoch 36/100 loss: 0.07009762875657374\n",
      "Epoch 37/100 loss: 0.06678477741000263\n",
      "Epoch 38/100 loss: 0.06360036727661252\n",
      "Epoch 39/100 loss: 0.06953979509779099\n",
      "Epoch 40/100 loss: 0.06515430807622608\n",
      "Epoch 41/100 loss: 0.062232263742024535\n",
      "Epoch 42/100 loss: 0.06285397911878592\n",
      "Epoch 43/100 loss: 0.06355768417500716\n",
      "Epoch 44/100 loss: 0.0634302101025748\n",
      "Epoch 45/100 loss: 0.06349722327945576\n",
      "Epoch 46/100 loss: 0.06179508400804933\n",
      "Epoch 47/100 loss: 0.06270342019168287\n",
      "Epoch 48/100 loss: 0.060208016075356474\n",
      "Epoch 49/100 loss: 0.06252252418531666\n",
      "Epoch 50/100 loss: 0.061940287675532925\n",
      "Epoch 51/100 loss: 0.06051617049222362\n",
      "Epoch 52/100 loss: 0.059989570387434944\n",
      "Epoch 53/100 loss: 0.06144765354874329\n",
      "Epoch 54/100 loss: 0.059383219232658414\n",
      "Epoch 55/100 loss: 0.060669072044390523\n",
      "Epoch 56/100 loss: 0.061174105266118765\n",
      "Epoch 57/100 loss: 0.058982614497304874\n",
      "Epoch 58/100 loss: 0.05905293996659259\n",
      "Epoch 59/100 loss: 0.05873917776707505\n",
      "Epoch 60/100 loss: 0.06037941934917995\n",
      "Epoch 61/100 loss: 0.05786965829095652\n",
      "Epoch 62/100 loss: 0.05921172577793688\n",
      "Epoch 63/100 loss: 0.061103984907971684\n",
      "Epoch 64/100 loss: 0.05885177927476452\n",
      "Epoch 65/100 loss: 0.05986461247727193\n",
      "Epoch 66/100 loss: 0.05885164959543603\n",
      "Epoch 67/100 loss: 0.060963784583812054\n",
      "Epoch 68/100 loss: 0.05784072909000895\n",
      "Epoch 69/100 loss: 0.0585056865073765\n",
      "Epoch 70/100 loss: 0.05599042078110365\n",
      "Epoch 71/100 loss: 0.05791668807065372\n",
      "Epoch 72/100 loss: 0.05649034934081408\n",
      "Epoch 73/100 loss: 0.05522426798242119\n",
      "Epoch 74/100 loss: 0.05686908941723109\n",
      "Epoch 75/100 loss: 0.05522525981744138\n",
      "Epoch 76/100 loss: 0.05774299876275354\n",
      "Epoch 77/100 loss: 0.0569402690875435\n",
      "Epoch 78/100 loss: 0.05615482838598965\n",
      "Epoch 79/100 loss: 0.05581823512366276\n",
      "Epoch 80/100 loss: 0.05527688578696499\n",
      "Epoch 81/100 loss: 0.058084327847313905\n",
      "Epoch 82/100 loss: 0.05558589996014219\n",
      "Epoch 83/100 loss: 0.05513747762370212\n",
      "Epoch 84/100 loss: 0.055531529506864595\n",
      "Epoch 85/100 loss: 0.054918979412103684\n",
      "Epoch 86/100 loss: 0.057157973772157825\n",
      "Epoch 87/100 loss: 0.0560107620406521\n",
      "Epoch 88/100 loss: 0.056876787111857674\n",
      "Epoch 89/100 loss: 0.05497751299723288\n",
      "Epoch 90/100 loss: 0.05607785604013197\n",
      "Epoch 91/100 loss: 0.05676145075376337\n",
      "Epoch 92/100 loss: 0.05627810053663645\n",
      "Epoch 93/100 loss: 0.05409595391874152\n",
      "Epoch 94/100 loss: 0.05628370720264049\n",
      "Epoch 95/100 loss: 0.05512863089545283\n",
      "Epoch 96/100 loss: 0.05357440678346145\n",
      "Epoch 97/100 loss: 0.053629620621887454\n",
      "Epoch 98/100 loss: 0.053890014849861896\n",
      "Epoch 99/100 loss: 0.05315125724003005\n",
      "Epoch 100/100 loss: 0.0541705971599172\n",
      "0.631578947368421\n",
      "Epoch 1/100 loss: 0.13786690149625974\n",
      "Epoch 2/100 loss: 0.10159290646831516\n",
      "Epoch 3/100 loss: 0.0929739060598395\n",
      "Epoch 4/100 loss: 0.0891479848557157\n",
      "Epoch 5/100 loss: 0.08426490568171925\n",
      "Epoch 6/100 loss: 0.0781873325031827\n",
      "Epoch 7/100 loss: 0.07884763898201692\n",
      "Epoch 8/100 loss: 0.07359746330743742\n",
      "Epoch 9/100 loss: 0.0743193826101975\n",
      "Epoch 10/100 loss: 0.07163057524272556\n",
      "Epoch 11/100 loss: 0.0723907567337981\n",
      "Epoch 12/100 loss: 0.07162429252728122\n",
      "Epoch 13/100 loss: 0.06926745924554463\n",
      "Epoch 14/100 loss: 0.06841304988946768\n",
      "Epoch 15/100 loss: 0.06737394580989849\n",
      "Epoch 16/100 loss: 0.06578387170711694\n",
      "Epoch 17/100 loss: 0.06711076784433051\n",
      "Epoch 18/100 loss: 0.06579485378147262\n",
      "Epoch 19/100 loss: 0.06411863640387781\n",
      "Epoch 20/100 loss: 0.06502679979728551\n",
      "Epoch 21/100 loss: 0.06322806055374149\n",
      "Epoch 22/100 loss: 0.06655547919680961\n",
      "Epoch 23/100 loss: 0.06242853541980514\n",
      "Epoch 24/100 loss: 0.06335033630222961\n",
      "Epoch 25/100 loss: 0.06294782087150745\n",
      "Epoch 26/100 loss: 0.05984898918998876\n",
      "Epoch 27/100 loss: 0.06091322818626667\n",
      "Epoch 28/100 loss: 0.061312233325255545\n",
      "Epoch 29/100 loss: 0.05906427072485608\n",
      "Epoch 30/100 loss: 0.06037191574868272\n",
      "Epoch 31/100 loss: 0.058680112419306\n",
      "Epoch 32/100 loss: 0.05799496590951412\n",
      "Epoch 33/100 loss: 0.058713738775330475\n",
      "Epoch 34/100 loss: 0.06031677255252685\n",
      "Epoch 35/100 loss: 0.06050436583965285\n",
      "Epoch 36/100 loss: 0.057044343851669795\n",
      "Epoch 37/100 loss: 0.05699584898077459\n",
      "Epoch 38/100 loss: 0.056201411963615173\n",
      "Epoch 39/100 loss: 0.05625539244739031\n",
      "Epoch 40/100 loss: 0.05667308408161818\n",
      "Epoch 41/100 loss: 0.054511140405267895\n",
      "Epoch 42/100 loss: 0.058961688622012245\n",
      "Epoch 43/100 loss: 0.05668452358978512\n",
      "Epoch 44/100 loss: 0.056038108190405125\n",
      "Epoch 45/100 loss: 0.054314461116202487\n",
      "Epoch 46/100 loss: 0.056079900396627996\n",
      "Epoch 47/100 loss: 0.05482753858346097\n",
      "Epoch 48/100 loss: 0.055902579505926024\n",
      "Epoch 49/100 loss: 0.05439531495725394\n",
      "Epoch 50/100 loss: 0.055036899012464384\n",
      "Epoch 51/100 loss: 0.05467666011841129\n",
      "Epoch 52/100 loss: 0.053972936256990504\n",
      "Epoch 53/100 loss: 0.054263985285780074\n",
      "Epoch 54/100 loss: 0.05531276971152395\n",
      "Epoch 55/100 loss: 0.055455092185239495\n",
      "Epoch 56/100 loss: 0.0532806632897076\n",
      "Epoch 57/100 loss: 0.050890475155157995\n",
      "Epoch 58/100 loss: 0.056165654609665466\n",
      "Epoch 59/100 loss: 0.05465426985282562\n",
      "Epoch 60/100 loss: 0.05171500503645367\n",
      "Epoch 61/100 loss: 0.052258090636657095\n",
      "Epoch 62/100 loss: 0.05298197544998053\n",
      "Epoch 63/100 loss: 0.052140591132260716\n",
      "Epoch 64/100 loss: 0.05210310594889822\n",
      "Epoch 65/100 loss: 0.05294851457157571\n",
      "Epoch 66/100 loss: 0.05274635636280322\n",
      "Epoch 67/100 loss: 0.051003664154908944\n",
      "Epoch 68/100 loss: 0.050190811046883416\n",
      "Epoch 69/100 loss: 0.05270954457011601\n",
      "Epoch 70/100 loss: 0.050687090299932056\n",
      "Epoch 71/100 loss: 0.051102489457371995\n",
      "Epoch 72/100 loss: 0.05069429077109073\n",
      "Epoch 73/100 loss: 0.05166854702421885\n",
      "Epoch 74/100 loss: 0.050405605250344244\n",
      "Epoch 75/100 loss: 0.05035224165162702\n",
      "Epoch 76/100 loss: 0.04983112561350562\n",
      "Epoch 77/100 loss: 0.05144988057271768\n",
      "Epoch 78/100 loss: 0.05214345671216797\n",
      "Epoch 79/100 loss: 0.05014133834028136\n",
      "Epoch 80/100 loss: 0.05027380761392279\n",
      "Epoch 81/100 loss: 0.050509126825935065\n",
      "Epoch 82/100 loss: 0.050696300133798576\n",
      "Epoch 83/100 loss: 0.05010822151005908\n",
      "Epoch 84/100 loss: 0.05077206347847586\n",
      "Epoch 85/100 loss: 0.04916903680083746\n",
      "Epoch 86/100 loss: 0.049979803949275974\n",
      "Epoch 87/100 loss: 0.049398954408997206\n",
      "Epoch 88/100 loss: 0.04797362437217861\n",
      "Epoch 89/100 loss: 0.04926947361908296\n",
      "Epoch 90/100 loss: 0.04872279966301473\n",
      "Epoch 91/100 loss: 0.048979516909948416\n",
      "Epoch 92/100 loss: 0.05005788101211519\n",
      "Epoch 93/100 loss: 0.04746642135522031\n",
      "Epoch 94/100 loss: 0.04981997869924464\n",
      "Epoch 95/100 loss: 0.04909350947224161\n",
      "Epoch 96/100 loss: 0.050056999895949\n",
      "Epoch 97/100 loss: 0.04879713392885056\n",
      "Epoch 98/100 loss: 0.04665868484808656\n",
      "Epoch 99/100 loss: 0.047549149984575795\n",
      "Epoch 100/100 loss: 0.04777460823643878\n",
      "0.6842105263157894\n",
      "Epoch 1/100 loss: 0.22239480298819267\n",
      "Epoch 2/100 loss: 0.1512132687850077\n",
      "Epoch 3/100 loss: 0.14295198750544624\n",
      "Epoch 4/100 loss: 0.1337965526373413\n",
      "Epoch 5/100 loss: 0.13297579072787508\n",
      "Epoch 6/100 loss: 0.12917626784608408\n",
      "Epoch 7/100 loss: 0.12687763893597098\n",
      "Epoch 8/100 loss: 0.12531277082047992\n",
      "Epoch 9/100 loss: 0.12839325255040884\n",
      "Epoch 10/100 loss: 0.13146754034728714\n",
      "Epoch 11/100 loss: 0.12785126422066967\n",
      "Epoch 12/100 loss: 0.11578711803336193\n",
      "Epoch 13/100 loss: 0.09570061578373236\n",
      "Epoch 14/100 loss: 0.09074655593418635\n",
      "Epoch 15/100 loss: 0.08451297979066635\n",
      "Epoch 16/100 loss: 0.08269829988317738\n",
      "Epoch 17/100 loss: 0.08145006302826839\n",
      "Epoch 18/100 loss: 0.07642716153864704\n",
      "Epoch 19/100 loss: 0.07756813300149298\n",
      "Epoch 20/100 loss: 0.07459218692892026\n",
      "Epoch 21/100 loss: 0.07018583901744938\n",
      "Epoch 22/100 loss: 0.07094764008801728\n",
      "Epoch 23/100 loss: 0.06799191535384587\n",
      "Epoch 24/100 loss: 0.06865050613713061\n",
      "Epoch 25/100 loss: 0.06722286626013733\n",
      "Epoch 26/100 loss: 0.06506111812282865\n",
      "Epoch 27/100 loss: 0.06356746888312295\n",
      "Epoch 28/100 loss: 0.061298669668772655\n",
      "Epoch 29/100 loss: 0.062930081795414\n",
      "Epoch 30/100 loss: 0.06093147133396324\n",
      "Epoch 31/100 loss: 0.0594769084530296\n",
      "Epoch 32/100 loss: 0.059093091332028794\n",
      "Epoch 33/100 loss: 0.05812958463075279\n",
      "Epoch 34/100 loss: 0.058185136637008576\n",
      "Epoch 35/100 loss: 0.056449361943682275\n",
      "Epoch 36/100 loss: 0.056947656639948566\n",
      "Epoch 37/100 loss: 0.05706284448233174\n",
      "Epoch 38/100 loss: 0.056127872476826275\n",
      "Epoch 39/100 loss: 0.055286502932852595\n",
      "Epoch 40/100 loss: 0.056875519375422214\n",
      "Epoch 41/100 loss: 0.05614557357241884\n",
      "Epoch 42/100 loss: 0.05562401743554433\n",
      "Epoch 43/100 loss: 0.054094808084861556\n",
      "Epoch 44/100 loss: 0.05448699225009792\n",
      "Epoch 45/100 loss: 0.05450031334440753\n",
      "Epoch 46/100 loss: 0.055409347184611234\n",
      "Epoch 47/100 loss: 0.05182913609674962\n",
      "Epoch 48/100 loss: 0.05280421564957477\n",
      "Epoch 49/100 loss: 0.05324490021319097\n",
      "Epoch 50/100 loss: 0.05215474852801621\n",
      "Epoch 51/100 loss: 0.053297830126336675\n",
      "Epoch 52/100 loss: 0.054092277703342954\n",
      "Epoch 53/100 loss: 0.051321989483797854\n"
     ]
    }
   ],
   "source": [
    "task_1_accuracies = {}\n",
    "task_2_accuracies = {}\n",
    "\n",
    "task_1_losses = {}\n",
    "task_2_losses = {}\n",
    "\n",
    "for alpha in np.linspace(0, 1, 20):\n",
    "    print(alpha)\n",
    "    task_1_trained_network_copy = copy.deepcopy(task_1_trained_network)\n",
    "    task_1_trained_network_copy.switch(1)\n",
    "#     interpolated_dataset = get_dataset_mix(dataset_01, dataset_78, alpha)\n",
    "#     interpolated_dataloader = torch.utils.data.DataLoader(interpolated_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "#     interpolated_dataset_test = get_dataset_mix(dataset_01_test, dataset_78_test, alpha)\n",
    "#     interpolated_dataloader_test = torch.utils.data.DataLoader(interpolated_dataset_test, batch_size=len(interpolated_dataset_test), shuffle=True)\n",
    "    \n",
    "    losses, task_2_trained_network = train(dataloader_01, dataloader_78, alpha, Network(), 100, 0.01, target_mapping)\n",
    "    task_2_loss, task_2_accuracy = test(dataloader_01_test, dataloader_78_test, alpha, task_2_trained_network, target_mapping)\n",
    "    \n",
    "    task_2_trained_network.switch(0)\n",
    "    task_1_loss, task_1_accuracy = test(dataloader_01_test, dataloader_78_test, 1, task_2_trained_network, target_mapping)\n",
    "    \n",
    "#     task_2_test_data = next(iter(interpolated_dataloader_test))\n",
    "#     task_2_test_inputs = task_2_test_data[0]\n",
    "#     task_2_test_labels = task_2_test_data[1]\n",
    "#     task_2_test_labels_mapped = np.array([label_mapping[i] for i in task_2_test_labels.numpy()])\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         task_2_test_outputs = task_2_trained_network(task_2_test_inputs)\n",
    "    \n",
    "#     task_2_accuracy = sum(np.sign(task_2_test_outputs.numpy()).flatten() == task_2_test_labels_mapped) / len(task_2_test_inputs)\n",
    "    \n",
    "#     task_2_trained_network.switch(0)\n",
    "    \n",
    "#     task_1_test_data = next(iter(dataloader_01_test))\n",
    "#     task_1_test_inputs = task_1_test_data[0]\n",
    "#     task_1_test_labels = task_1_test_data[1]\n",
    "#     task_1_test_labels_mapped = np.array([label_mapping[i] for i in task_1_test_labels.numpy()])\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         task_1_test_outputs = task_2_trained_network(task_1_test_inputs)\n",
    "    \n",
    "#     task_1_accuracy = sum(np.sign(task_1_test_outputs.numpy()).flatten() == task_1_test_labels_mapped) / len(task_1_test_inputs)\n",
    "    \n",
    "#     print(task_1_accuracy)\n",
    "#     print(task_2_accuracy)\n",
    "    \n",
    "    task_2_accuracies[alpha] = task_2_accuracy\n",
    "    task_1_accuracies[alpha] = task_1_accuracy\n",
    "    \n",
    "    task_1_losses[alpha] = task_1_loss\n",
    "    task_2_losses[alpha] = task_2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "afraid-brass",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 0.503,\n",
       " 0.1111111111111111: 0.9235,\n",
       " 0.2222222222222222: 0.969,\n",
       " 0.3333333333333333: 0.9675,\n",
       " 0.4444444444444444: 0.9815,\n",
       " 0.5555555555555556: 0.975,\n",
       " 0.6666666666666666: 0.984,\n",
       " 0.7777777777777777: 0.9825,\n",
       " 0.8888888888888888: 0.9875,\n",
       " 1.0: 0.988}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_1_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "occasional-journal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 0.9975,\n",
       " 0.1111111111111111: 0.9616724738675958,\n",
       " 0.2222222222222222: 0.9699849924962481,\n",
       " 0.3333333333333333: 0.9803052683407188,\n",
       " 0.4444444444444444: 0.9850894632206759,\n",
       " 0.5555555555555556: 0.9824390243902439,\n",
       " 0.6666666666666666: 0.9798285426122038,\n",
       " 0.7777777777777777: 0.9831181727904668,\n",
       " 0.8888888888888888: 0.986815415821501,\n",
       " 1.0: 0.9845}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_2_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ideal-zambia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1243cb340>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(next(di)[0].numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "foster-behalf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(264.)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(torch.bernoulli(0.5 * torch.ones(500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "sticky-option",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-159-82b3b645d734>:4: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(task_1_accuracies.keys(), task_1_accuracies.values())\n",
    "plt.plot(task_2_accuracies.keys(), task_2_accuracies.values())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-stage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cata",
   "language": "python",
   "name": "cata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

experiment_name:                    
use_gpu:                            True                      
seed:                               5                         

task:
  label_task_boundaries:            True                      
  learner_configuration:            continual                 # meta or continual - specifies whether student has separate heads for each teacher (continual) or not (meta)
  teacher_configuration:            overlapping               # noisy, independent, overlapping, drifting, trained_mnist (see README)
  num_teachers:                     2                         
  loss_type:                        regression                # classification or regression

training:
  total_training_steps:                
  train_batch_size:                 1                         
  learning_rate:                    0.05                         
  weight_decay:                     0                      
  loss_function:                    mse                       # mse, bce
  scale_head_lr:            True                      # whether to add 1/N factor in lr of output layer  

data:
  input_source:                     mnist_stream              # iid_gaussian, mnist (see README)
  same_input_distribution:          True

logging:
  verbose:                          True                      
  verbose_tb:                       1                     
  checkpoint_frequency:             5000                      
  log_to_df:                        True
  merge_at_checkpoint:              False
  
testing:
  test_batch_size:                  50000                     # generalisation error
  test_frequency:                   1                         # how often during training to perform generalisation error test loop
  overlap_frequency:                100                       # how often during training to compute / visualise overlap matrices
  test_all_teachers:                True                      # whether to compute generalisation losses for all teachers or just one currently teaching

model:
  input_dimension:                  784                       
  student_hidden_layers:            [2]                       
  teacher_hidden_layers:            [1]                       
  output_dimension:                 1                         
  student_nonlinearity:             relu                      
  teacher_nonlinearities:           [relu, relu]              # per teacher
  teacher_initialisation_std:       1                         # std of normal initialisation for teacher network
  student_initialisation_std:       0.001                     # std of normal initialisation for student network
  initialise_student_outputs:       False                     # whether or not to initialise hidden -> output weights of student
  soft_committee:                   False                     # whether or not to freeze output layer (scm)
  teacher_bias_parameters:          False                     # whether or not to have bias parameters on linaer layers (teacher)
  student_bias_parameters:          False                     # whether or not to have bias parameters on linaer layers (student)

curriculum:
  type:                             custom                    # curriculum type (custom - declarative or standard - imperative)
  selection_type:                   cyclical                  # how to determine next task (random or cyclical)
  stopping_condition:               threshold_sequence        # condition on which to switch task (fixed_period or threshold)
  fixed_period:                     5000                      # period for changing tasks 
  loss_threshold:                   [0.0001, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001, 0.000000001, 0.000000001, 0.0000000001]                    # loss threshold under which teacher is changed
  custom:                           [0, 1]                    # curriculum defined manually. Each entry defines next teacher index

teachers:
  overlap_percentages:              [0, 0]                    # percentage of weights to copy between teachers per layer. 
  teacher_noise:                    [0, 0]

drift_teachers:
  drift_size:                       0.01
  drift_frequency:                  1

mnist_data:
  data_path:                        "../../data/"
  pca_input:                        -1                        # number of principle components (no PCA if -1, not applicable to iid_gaussian input)                         
  standardise:                      True
  noise:

pure_mnist:
  teacher_digits:                   [[0, 1], [4, 5]]
  rotations:                        [[0, 0], [90, 90]]

trained_mnist:
  save_weight_path:                 "../saved_model_weights/"
  convergence_criterion:            0.001
  learning_rate:                    0.001
  output_dimension:                 1
  batch_size:                       32

post_processing:
  combine_plots:                    True
  show_legends:                     True
  crop_x:                           
  plot_width:                       5
  plot_height:                      4
  plot_thickness:                   1.2